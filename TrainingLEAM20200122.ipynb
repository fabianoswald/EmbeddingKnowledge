{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T12:36:40.060188Z",
     "start_time": "2020-01-23T12:36:40.053413Z"
    }
   },
   "source": [
    "**Master Thesis Fabian Asal; Training and Evaluation LEAM**\n",
    "\n",
    "- Title: Leveraging Label Descriptions to Improve Multi-Label Text Classification for Short Noisy Medical Text\n",
    "- Date: 22.01.2020\n",
    "- Copyright LEAM: Guoyin Wang, Chunyuan Li, Wenlin Wang, Yizhe Zhang, Dinghan Shen, Xinyuan Zhang, Ricardo Henao, Lawrence Carin\n",
    "- Link to LEAM: https://github.com/guoyinwang/LEAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Load\" data-toc-modified-id=\"Load-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Load</a></span><ul class=\"toc-item\"><li><span><a href=\"#Utilities\" data-toc-modified-id=\"Utilities-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Utilities</a></span></li><li><span><a href=\"#Datasets\" data-toc-modified-id=\"Datasets-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Datasets</a></span></li></ul></li><li><span><a href=\"#LEAM\" data-toc-modified-id=\"LEAM-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>LEAM</a></span><ul class=\"toc-item\"><li><span><a href=\"#Configuration\" data-toc-modified-id=\"Configuration-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Configuration</a></span></li><li><span><a href=\"#Model\" data-toc-modified-id=\"Model-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Model</a></span></li><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Evaluation</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/fabianoswald/EmbeddingKnowledge/blob/master/evalLEAM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "ghtzLr5P2ZVT",
    "outputId": "0dd49365-dcb9-4627-8919-4d3d191fad31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J0EQ9mEa2ytu"
   },
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "KTcevX4g3S8s",
    "outputId": "ecca18e2-f210-4b85-f91b-fab7f863abec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==1.7.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/13/eb2c71c340e2885feb5d562c337272d9ec1b300642b7298b10924e3db098/tensorflow-1.7.0-cp27-cp27mu-manylinux1_x86_64.whl (48.0MB)\n",
      "\u001b[K     |████████████████████████████████| 48.0MB 62kB/s \n",
      "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.7.0) (1.15.0)\n",
      "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.7.0) (2.0.0)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.7.0) (3.7.1)\n",
      "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.7.0) (0.2.2)\n",
      "Collecting tensorboard<1.8.0,>=1.7.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/5b/18f50b69b8af42f93c47cd8bf53337347bc1974480a10de51fdd7f8fd48b/tensorboard-1.7.0-py2-none-any.whl (3.1MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1MB 55.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.7.0) (0.33.6)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.7.0) (0.7.1)\n",
      "Requirement already satisfied: backports.weakref>=1.0rc1 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.7.0) (1.0.post1)\n",
      "Requirement already satisfied: enum34>=1.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.7.0) (1.1.6)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.7.0) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.7.0) (1.16.4)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.7.0) (1.1.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.7.0) (0.8.0)\n",
      "Requirement already satisfied: futures>=2.2.0 in /usr/local/lib/python2.7/dist-packages (from grpcio>=1.8.6->tensorflow==1.7.0) (3.2.0)\n",
      "Requirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow==1.7.0) (1.0.2)\n",
      "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow==1.7.0) (5.4.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python2.7/dist-packages (from protobuf>=3.4.0->tensorflow==1.7.0) (42.0.2)\n",
      "Collecting bleach==1.5.0\n",
      "  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
      "Collecting html5lib==0.9999999\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)\n",
      "\u001b[K     |████████████████████████████████| 890kB 48.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.8.0,>=1.7.0->tensorflow==1.7.0) (0.15.5)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.8.0,>=1.7.0->tensorflow==1.7.0) (3.1.1)\n",
      "Building wheels for collected packages: html5lib\n",
      "  Building wheel for html5lib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for html5lib: filename=html5lib-0.9999999-cp27-none-any.whl size=107220 sha256=99bfa9d42b2f3c34979f635946746922c742e8f9e5e7d56c636bbda5a9ddd24d\n",
      "  Stored in directory: /root/.cache/pip/wheels/50/ae/f9/d2b189788efcf61d1ee0e36045476735c838898eef1cad6e29\n",
      "Successfully built html5lib\n",
      "\u001b[31mERROR: fastai 0.7.0 has requirement torch<0.4, but you'll have torch 1.3.1+cu100 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: magenta 0.3.19 has requirement tensorflow>=1.12.0, but you'll have tensorflow 1.7.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: html5lib, bleach, tensorboard, tensorflow\n",
      "  Found existing installation: html5lib 1.0.1\n",
      "    Uninstalling html5lib-1.0.1:\n",
      "      Successfully uninstalled html5lib-1.0.1\n",
      "  Found existing installation: bleach 3.1.0\n",
      "    Uninstalling bleach-3.1.0:\n",
      "      Successfully uninstalled bleach-3.1.0\n",
      "  Found existing installation: tensorboard 1.15.0\n",
      "    Uninstalling tensorboard-1.15.0:\n",
      "      Successfully uninstalled tensorboard-1.15.0\n",
      "  Found existing installation: tensorflow 1.15.0\n",
      "    Uninstalling tensorflow-1.15.0:\n",
      "      Successfully uninstalled tensorflow-1.15.0\n",
      "Successfully installed bleach-1.5.0 html5lib-0.9999999 tensorboard-1.7.0 tensorflow-1.7.0\n",
      "Collecting chars2vec\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/0a/8c327aae23e0532d239ec7b30446aca765eb5d9547b4c4b09cdd82e49797/chars2vec-0.1.7.tar.gz (8.1MB)\n",
      "\u001b[K     |████████████████████████████████| 8.1MB 4.1MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: chars2vec\n",
      "  Building wheel for chars2vec (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for chars2vec: filename=chars2vec-0.1.7-cp27-none-any.whl size=8111097 sha256=c4129bfdd3aa2537a2f7b3b7e3972275d7b88a8c2cb8ab1cd0e9bbc9ff8ea3fb\n",
      "  Stored in directory: /root/.cache/pip/wheels/97/b6/65/d7e778ef1213ec77d315aea0f536068b96e36cc94c02abbfde\n",
      "Successfully built chars2vec\n",
      "Installing collected packages: chars2vec\n",
      "Successfully installed chars2vec-0.1.7\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==1.7.0\n",
    "!pip install chars2vec\n",
    "#!pip install keras==2.1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "R7oXY3kD2je1",
    "outputId": "805b8120-d730-4387-aaef-484ae4826461"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import os, sys, pickle, re, string, itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score, f1_score, hamming_loss, accuracy_score\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "import csv\n",
    "import pdb\n",
    "import operator\n",
    "\n",
    "import chars2vec\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import layers\n",
    "import scipy.io as sio\n",
    "from math import floor\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "6aD4yUlc3ZmL",
    "outputId": "3c791a7f-1304-4b7e-9b56-e83f6a631770"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 2.7.17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.7.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python --version\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fJLLL7TPC8rb"
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "zero_shot_dec06 = \"/content/drive/My Drive/data/aok/zero_shot_dec06.p\"\n",
    "zero_shot_dec06_emb= \"/content/drive/My Drive/data/aok/zero_shot_dec06_emb.p\"\n",
    "\n",
    "multi_class_dec06 = \"/content/drive/My Drive/data/aok/multi_xx_dec06.p\"\n",
    "multi_class_dec06_emb= \"/content/drive/My Drive/data/aok/multi_xx_dec06_emb.p\"\n",
    "\n",
    "dataset2_dec06 = \"/content/drive/My Drive/data/aok/noisy_13_dec06.p\"\n",
    "dataset2_dec06_emb= \"/content/drive/My Drive/data/aok/noisy_13_dec06_emb.p\"\n",
    "\n",
    "dataset1_dec06 = \"/content/drive/My Drive/data/aok/noisy_63_dec06.p\"\n",
    "dataset1_dec06_emb= \"/content/drive/My Drive/data/aok/noisy_63_dec06_emb.p\"\n",
    "\n",
    "raw200_dec06 = \"/content/drive/My Drive/data/aok/raw200_dec06.p\"\n",
    "raw200_dec06_emb= \"/content/drive/My Drive/data/aok/raw200_dec06_emb.p\"\n",
    "\n",
    "dec06 = \"/content/drive/My Drive/data/aok/dec06.p\"\n",
    "dec06_emb= \"/content/drive/My Drive/data/aok/dec06_emb.p\"\n",
    "\n",
    "x = pickle.load(open(dataset2_dec06, \"rb\"))\n",
    "\n",
    "train, train_text, train_lab = x[0],x[1], x[2]\n",
    "val, val_text, val_lab = x[3],x[4], x[5]\n",
    "test, test_text, test_lab = x[6],x[7], x[8]\n",
    "wordtoix, ixtoword, description_clean = x[9], x[10], x[11]\n",
    "\n",
    "# load embeddings\n",
    "char_embeddings = pickle.load(open(dataset2_dec06_emb, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ohmIsR6l7ftf",
    "outputId": "73743187-972f-4b60-ede4-430ee7373997"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1499)\n"
     ]
    }
   ],
   "source": [
    "a=0;b=0;\n",
    "\n",
    "for arr in val_lab:\n",
    "    if len(np.unique(arr)) != 2:\n",
    "        a+=1\n",
    "    else:\n",
    "        b+=1\n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dpw3RS3-jmc2"
   },
   "source": [
    "# LEAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lee1XMmnjic6"
   },
   "outputs": [],
   "source": [
    "class Options(object):\n",
    "    def __init__(self):\n",
    "        self.GPUID = 0\n",
    "        self.dataset = 'aok'\n",
    "        self.fix_emb = True\n",
    "        self.restore = True\n",
    "        self.W_emb = None\n",
    "        self.W_class_emb = None\n",
    "        self.maxlen = 305\n",
    "        self.n_words = None\n",
    "        self.embed_size = 300\n",
    "        self.lr = 1e-3\n",
    "        self.batch_size = 10\n",
    "        self.max_epochs = 10\n",
    "        self.dropout = 0.5\n",
    "        self.part_data = False\n",
    "        self.portion = 1.0\n",
    "        self.save_path = \"./save/\"\n",
    "        self.log_path = \"./log/\"\n",
    "        self.print_freq = 100\n",
    "        self.valid_freq = 100\n",
    "\n",
    "        self.optimizer = 'Adam'\n",
    "        self.clip_grad = None\n",
    "        self.class_penalty = 1.0\n",
    "        self.ngram = 2\n",
    "        self.H_dis = 300\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        for attr, value in self.__dict__.iteritems():\n",
    "            yield attr, value\n",
    "\n",
    "opt= Options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uAGbKaSgj4GV"
   },
   "outputs": [],
   "source": [
    "emb_file= char_embeddings\n",
    "opt.num_class = len(description_clean)\n",
    "opt.class_name= description_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "rVHoBe2O4gsk",
    "outputId": "7516b3c3-7fa4-460b-fc4a-018286ade9fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'restore': True, 'fix_emb': True, 'log_path': './log/', 'dataset': 'aok', 'part_data': False, 'max_epochs': 10, 'clip_grad': None, 'embed_size': 300, 'class_name': [u'anziehen ausziehen von kompressionsstruempfen', u'injektion', u'abnehmen anlegen von kompressionsverbaenden', u'dekubitusbehandlung', u'weiterleiten sachbearbeiter', u'arzneien verabreichen und ueberwachen', u'gabe von salbe und tropfen augen', u'blutzucker messen', u'einreibungen', u'insulininjektion', u'anlegen und wechseln von wundverbaenden', u'medikamentenabgabe mittels herz schmerz hormonpflaster', u'richten von medikamenten wochendosimed'], 'lr': 0.001, 'save_path': './save/', 'W_class_emb': None, 'class_penalty': 1.0, 'optimizer': 'Adam', 'valid_freq': 100, 'dropout': 0.5, 'batch_size': 10, 'ngram': 2, 'GPUID': 0, 'n_words': 12408, 'H_dis': 300, 'W_emb': None, 'num_class': 13, 'print_freq': 100, 'maxlen': 305, 'portion': 1.0}\n",
      "Total words: 12408\n"
     ]
    }
   ],
   "source": [
    "train_lab = np.array(train_lab, dtype='float32')\n",
    "val_lab = np.array(val_lab, dtype='float32')\n",
    "test_lab = np.array(test_lab, dtype='float32')\n",
    "opt.n_words = len(ixtoword)\n",
    "if opt.part_data:\n",
    "    #np.random.seed(123)\n",
    "    train_ind = np.random.choice(len(train), int(len(train)*opt.portion), replace=False)\n",
    "    train = [train[t] for t in train_ind]\n",
    "    train_lab = [train_lab[t] for t in train_ind]\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(opt.GPUID)\n",
    "\n",
    "print(dict(opt))\n",
    "print('Total words: %d' % opt.n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aRP7MhNj4gwr"
   },
   "outputs": [],
   "source": [
    "def load_class_embedding( wordtoidx, opt):\n",
    "    print(\"load class embedding\")\n",
    "    name_list = [ k.lower().split(' ') for k in opt.class_name]\n",
    "    id_list = [ [ wordtoix[i] for i in l] for l in name_list]\n",
    "    value_list = [ [ opt.W_emb[i] for i in l]    for l in id_list]\n",
    "    value_mean = [ np.mean(l,0)  for l in value_list]\n",
    "    return np.asarray(value_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "4qQMPDhu4gvS",
    "outputId": "a90fad95-5914-4fbb-b850-bbc8cd6ed5a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load class embedding\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    #opt.W_emb = np.array(cPickle.load(open(embpath, 'rb')),dtype='float32')\n",
    "    #opt.W_emb = np.squeeze(opt.W_emb, axis=0)\n",
    "    opt.W_emb= emb_file\n",
    "    opt.W_class_emb =  load_class_embedding( wordtoix, opt)\n",
    "except IOError:\n",
    "    print('No embedding file found.')\n",
    "    opt.fix_emb = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "isPnfhqmVxEA",
    "outputId": "57ecdfc9-c79c-4081-a03c-9e758d1c745b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12408, 300)"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.W_emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TCu1r8AxVCVn"
   },
   "outputs": [],
   "source": [
    "  def embedding(features, opt, prefix='', is_reuse=None):\n",
    "    \"\"\"Customized function to transform batched x into embeddings.\"\"\"\n",
    "    # Convert indexes of words into embeddings.\n",
    "    with tf.variable_scope(prefix + 'embed', reuse=tf.AUTO_REUSE):\n",
    "        if opt.fix_emb:\n",
    "            assert (hasattr(opt, 'W_emb'))\n",
    "            assert (np.shape(np.array(opt.W_emb)) == (opt.n_words, opt.embed_size))\n",
    "            W = tf.get_variable('W', initializer=opt.W_emb, trainable=True)\n",
    "            print(\"initialize word embedding finished\")\n",
    "        else:\n",
    "            weightInit = tf.random_uniform_initializer(-0.001, 0.001)\n",
    "            W = tf.get_variable('W', [opt.n_words, opt.embed_size], initializer=weightInit)\n",
    "    if hasattr(opt, 'relu_w') and opt.relu_w:\n",
    "        W = tf.nn.relu(W)\n",
    "    \n",
    "    word_vectors = tf.nn.embedding_lookup(W, features)\n",
    "\n",
    "    return word_vectors, W\n",
    "\n",
    "def embedding_class(features, opt, prefix='', is_reuse=None):\n",
    "    \"\"\"Customized function to transform batched y into embeddings.\"\"\"\n",
    "    # Convert indexes of words into embeddings.\n",
    "    with tf.variable_scope(prefix + 'embed', reuse=tf.AUTO_REUSE):\n",
    "        if opt.fix_emb:\n",
    "            assert (hasattr(opt, 'W_class_emb'))\n",
    "            W = tf.get_variable('W_class', initializer=opt.W_class_emb, trainable=True)\n",
    "            print(\"initialize class embedding finished\")\n",
    "        else:\n",
    "            weightInit = tf.random_uniform_initializer(-0.001, 0.001)\n",
    "            W = tf.get_variable('W_class', [opt.num_class, opt.embed_size], initializer=weightInit)\n",
    "    if hasattr(opt, 'relu_w') and opt.relu_w:\n",
    "        W = tf.nn.relu(W)\n",
    "    word_vectors = tf.nn.embedding_lookup(W, features)\n",
    "\n",
    "    return word_vectors, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eNA2rKLobTTp"
   },
   "outputs": [],
   "source": [
    "def partial_softmax(logits, weights, dim, name,):\n",
    "    with tf.name_scope('partial_softmax'):\n",
    "        exp_logits = tf.exp(logits)\n",
    "        if len(exp_logits.get_shape()) == len(weights.get_shape()):\n",
    "            exp_logits_weighted = tf.multiply(exp_logits, weights)\n",
    "        else:\n",
    "            exp_logits_weighted = tf.multiply(exp_logits, tf.expand_dims(weights, -1))\n",
    "        exp_logits_sum = tf.reduce_sum(exp_logits_weighted, axis=dim, keepdims=True)\n",
    "        partial_softmax_score = tf.div(exp_logits_weighted, exp_logits_sum, name=name)\n",
    "        return partial_softmax_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Jjj770TbTpD"
   },
   "outputs": [],
   "source": [
    "def att_emb_ngram_encoder_cnn(x_emb, x_mask, W_class, W_class_tran, opt):\n",
    "    x_mask = tf.expand_dims(x_mask, axis=-1) # b * s * 1\n",
    "    x_emb_0 = tf.squeeze(x_emb,) # b * s * e\n",
    "    x_emb_1 = tf.multiply(x_emb_0, x_mask) # b * s * e\n",
    "\n",
    "    H = tf.contrib.layers.conv2d(x_emb_0, num_outputs=opt.embed_size,kernel_size=[10], padding='SAME',activation_fn=tf.nn.relu) #b * s *  c\n",
    "\n",
    "\n",
    "    G = tf.contrib.keras.backend.dot(H, W_class_tran) # b * s * c\n",
    "    Att_v_max = partial_softmax(G, x_mask, 1, 'Att_v_max') # b * s * c\n",
    "\n",
    "    x_att = tf.contrib.keras.backend.batch_dot(tf.transpose(H,[0,2,1]), Att_v_max)\n",
    "    H_enc = tf.squeeze(x_att)\n",
    "    return H_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZSiQDAnVbTrt"
   },
   "outputs": [],
   "source": [
    "def discriminator_0layer(H, opt, dropout, prefix='', num_outputs=1, is_reuse=None):\n",
    "    H = tf.squeeze(H)\n",
    "    biasInit = tf.constant_initializer(0.001, dtype=tf.float32)\n",
    "    logits = layers.linear(tf.nn.dropout(H, keep_prob=dropout), num_outputs=num_outputs, biases_initializer=biasInit,\n",
    "                           scope=prefix + 'dis', reuse=tf.AUTO_REUSE)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9S3lEuZe_zbS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uBh7xdyHbTwo"
   },
   "outputs": [],
   "source": [
    "def emb_classifier(x, x_mask, y, dropout, opt, class_penalty):\n",
    "    # comment notation\n",
    "    #  b: batch size, s: sequence length, e: embedding dim, c : num of class\n",
    "    x_emb, W_norm = embedding(x, opt)  #  b * s * e\n",
    "    x_emb=tf.cast(x_emb,tf.float32)\n",
    "    W_norm=tf.cast(W_norm,tf.float32)\n",
    "    y_pos = tf.argmax(y, -1)\n",
    "    y_emb, W_class = embedding_class(y_pos, opt, 'class_emb') # b * e, c * e\n",
    "    y_emb=tf.cast(y_emb,tf.float32)\n",
    "    W_class=tf.cast(W_class,tf.float32)\n",
    "    W_class_tran = tf.transpose(W_class, [1,0]) # e * c\n",
    "    x_emb = tf.expand_dims(x_emb, 3)  # b * s * e * 1\n",
    "    H_enc = att_emb_ngram_encoder_cnn(x_emb, x_mask, W_class, W_class_tran, opt)\n",
    "\n",
    "    H_enc_list= tf.unstack(H_enc, axis=-1)\n",
    "    logits_list = []\n",
    "    for i, ih in enumerate(H_enc_list):\n",
    "        logits_list.append(discriminator_0layer(ih, opt, dropout, prefix='classify_{}'.format(i), num_outputs=1, is_reuse=False) )\n",
    "\n",
    "    logits = tf.concat(logits_list,-1)\n",
    "    prob = tf.nn.softmax(logits)\n",
    "    # class_y = tf.constant(name='class_y', shape=[opt.num_class, opt.num_class], dtype=tf.float32, value=np.identity(opt.num_class),)\n",
    "    correct_prediction = tf.equal(tf.argmax(prob, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    train_op = layers.optimize_loss(\n",
    "        loss,\n",
    "        global_step=global_step,\n",
    "        optimizer=opt.optimizer,\n",
    "        learning_rate=opt.lr)\n",
    "\n",
    "    return accuracy, loss, train_op, W_norm, global_step, logits, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "kXSK3hWfbTu9",
    "outputId": "9f7f1191-ac14-42ed-cfee-249a89927e3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize word embedding finished\n",
      "initialize class embedding finished\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n"
     ]
    }
   ],
   "source": [
    "    with tf.device('/gpu:1'):\n",
    "        x_ = tf.placeholder(tf.int32, shape=[opt.batch_size, opt.maxlen],name='x_')\n",
    "        x_mask_ = tf.placeholder(tf.float32, shape=[opt.batch_size, opt.maxlen],name='x_mask_')\n",
    "        keep_prob = tf.placeholder(tf.float32,name='keep_prob')\n",
    "        y_ = tf.placeholder(tf.float32, shape=[opt.batch_size, opt.num_class],name='y_')\n",
    "        class_penalty_ = tf.placeholder(tf.float32, shape=())\n",
    "        accuracy_, loss_, train_op, W_norm_, global_step, logits_, prob_ = emb_classifier(x_, x_mask_, y_, keep_prob, opt, class_penalty_)\n",
    "    uidx = 0\n",
    "    max_val_accuracy = 0.\n",
    "    max_test_accuracy = 0.\n",
    "    max_val_auc_mean = 0.\n",
    "    max_test_auc_mean = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zf1BLhKJbZ-W"
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto(log_device_placement=False, allow_soft_placement=True, )\n",
    "config.gpu_options.allow_growth = True\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gki4Os1NbaDv"
   },
   "outputs": [],
   "source": [
    "def get_minibatches_idx(n, minibatch_size, shuffle=False):\n",
    "    idx_list = np.arange(n, dtype=\"int32\")\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx_list)\n",
    "\n",
    "    minibatches = []\n",
    "    minibatch_start = 0\n",
    "    for i in range(n // minibatch_size):\n",
    "        minibatches.append(idx_list[minibatch_start:\n",
    "                                    minibatch_start + minibatch_size])\n",
    "        minibatch_start += minibatch_size\n",
    "    return zip(range(len(minibatches)), minibatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iLnSysgSbaHA"
   },
   "outputs": [],
   "source": [
    "def prepare_data_for_emb(seqs_x, opt):\n",
    "    maxlen = opt.maxlen\n",
    "    lengths_x = [len(s) for s in seqs_x]\n",
    "    if maxlen != None:\n",
    "        new_seqs_x = []\n",
    "        new_lengths_x = []\n",
    "        for l_x, s_x in zip(lengths_x, seqs_x):\n",
    "            if l_x < maxlen:\n",
    "                new_seqs_x.append(s_x)\n",
    "                new_lengths_x.append(l_x)\n",
    "            else:\n",
    "                new_seqs_x.append(s_x[:maxlen])\n",
    "                new_lengths_x.append(maxlen)\n",
    "        lengths_x = new_lengths_x\n",
    "        seqs_x = new_seqs_x\n",
    "\n",
    "        if len(lengths_x) < 1:\n",
    "            return None, None\n",
    "\n",
    "    n_samples = len(seqs_x)\n",
    "    maxlen_x = np.max(lengths_x)\n",
    "    x = np.zeros((n_samples, maxlen)).astype('int32')\n",
    "    x_mask = np.zeros((n_samples, maxlen)).astype('float32')\n",
    "    for idx, s_x in enumerate(seqs_x):\n",
    "        x[idx, :lengths_x[idx]] = s_x\n",
    "        x_mask[idx, :lengths_x[idx]] = 1. # change to remove the real END token\n",
    "    return x, x_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mFasvHgbbfs2"
   },
   "outputs": [],
   "source": [
    "def intersect_size(yhat, y, axis):\n",
    "    #axis=0 for label-level union (macro). axis=1 for instance-level\n",
    "    return np.logical_and(yhat, y).sum(axis=axis).astype(float)\n",
    "def macro_precision(yhat, y):\n",
    "    num = intersect_size(yhat, y, 0) / (yhat.sum(axis=0) + 1e-10)\n",
    "    return np.mean(num)\n",
    "\n",
    "def macro_recall(yhat, y):\n",
    "    num = intersect_size(yhat, y, 0) / (y.sum(axis=0) + 1e-10)\n",
    "    return np.mean(num)\n",
    "def macro_f1(yhat, y):\n",
    "    prec = macro_precision(yhat, y)\n",
    "    rec = macro_recall(yhat, y)\n",
    "    if prec + rec == 0:\n",
    "        f1 = 0.\n",
    "    else:\n",
    "        \n",
    "        f1 = 2*(prec*rec)/(prec+rec)\n",
    "    return f1\n",
    "\n",
    "def precision_at_k(yhat_raw, y, k):\n",
    "    #num true labels in top k predictions / k\n",
    "    sortd = np.argsort(yhat_raw)[:,::-1]\n",
    "    topk = sortd[:,:k]\n",
    "\n",
    "    #get precision at k for each example\n",
    "    vals = []\n",
    "    for i, tk in enumerate(topk):\n",
    "        if len(tk) > 0:\n",
    "            num_true_in_top_k = y[i,tk].sum()\n",
    "            denom = len(tk)\n",
    "            vals.append(num_true_in_top_k / float(denom))\n",
    "\n",
    "    return np.mean(vals)\n",
    "\n",
    "def micro_precision(yhatmic, ymic):\n",
    "    return intersect_size(yhatmic, ymic, 0) / yhatmic.sum(axis=0)\n",
    "\n",
    "def micro_recall(yhatmic, ymic):\n",
    "    return intersect_size(yhatmic, ymic, 0) / ymic.sum(axis=0)\n",
    "\n",
    "def micro_f1(yhatmic, ymic):\n",
    "    prec = micro_precision(yhatmic, ymic)\n",
    "    rec = micro_recall(yhatmic, ymic)\n",
    "\n",
    "    if prec + rec == 0:\n",
    "        f1 = 0.\n",
    "    else:\n",
    "        f1 = 2*(prec*rec)/(prec+rec)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kPuzzHHzp4j4"
   },
   "outputs": [],
   "source": [
    "def tensors_key_in_file(file_name):\n",
    "    \"\"\"Return tensors key in a checkpoint file.\n",
    "    Args:\n",
    "    file_name: Name of the checkpoint file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        reader = pywrap_tensorflow.NewCheckpointReader(file_name)\n",
    "        return reader.get_variable_to_shape_map()\n",
    "    except Exception as e:  # pylint: disable=broad-except\n",
    "        print(str(e))\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RFeUjpq3UeHW"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Z_b4--irScIv",
    "outputId": "80cd114e-24f2-4f61-943a-47bc7ec29e64"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/content/save/dataset2'"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.save_path= '/content/save/dataset2'\n",
    "opt.save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "q3ItRwakqZHY",
    "outputId": "2fa23e6a-67e0-46b5-ea51-3648f8ef7ed5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global name 'pywrap_tensorflow' is not defined\n",
      "No saving session, using random initialization\n",
      "Starting epoch 0\n",
      "Iteration 100: Training loss 0.206641 \n",
      "Train accuracy 0.540000 \n",
      "val auc macro 0.808566 micro 0.883705 \n",
      "Validation accuracy 0.510340 \n",
      "val hamming 0.074239 \n",
      "val accuracy 0.325503 total 485.000000\n",
      "Test auc macro 0.808160 micro 0.867960 \n",
      "Test f1 macro 0.203472 micro 0.414783 \n",
      "Test hamming 0.084202 \n",
      "Test accuracy 0.285235 accuracy total 425.000000 \n",
      "P5 0.232886\n",
      "Iteration 200: Training loss 0.235307 \n",
      "Train accuracy 0.610000 \n",
      "val auc macro 0.877309 micro 0.920201 \n",
      "Validation accuracy 0.611074 \n",
      "val hamming 0.063758 \n",
      "val accuracy 0.428188 total 638.000000\n",
      "Test auc macro 0.875029 micro 0.913452 \n",
      "Test f1 macro 0.316216 micro 0.510862 \n",
      "Test hamming 0.076717 \n",
      "Test accuracy 0.363087 accuracy total 541.000000 \n",
      "P5 0.251141\n",
      "Iteration 300: Training loss 0.167958 \n",
      "Train accuracy 0.640000 \n",
      "val auc macro 0.889564 micro 0.937278 \n",
      "Validation accuracy 0.620414 \n",
      "val hamming 0.059577 \n",
      "val accuracy 0.463758 total 691.000000\n",
      "Test auc macro 0.890963 micro 0.928161 \n",
      "Test f1 macro 0.375003 micro 0.548765 \n",
      "Test hamming 0.071657 \n",
      "Test accuracy 0.396644 accuracy total 591.000000 \n",
      "P5 0.255168\n",
      "Iteration 400: Training loss 0.225458 \n",
      "Train accuracy 0.660000 \n",
      "val auc macro 0.916000 micro 0.951027 \n",
      "Validation accuracy 0.635757 \n",
      "val hamming 0.055188 \n",
      "val accuracy 0.504027 total 751.000000\n",
      "Test auc macro 0.911346 micro 0.937198 \n",
      "Test f1 macro 0.425380 micro 0.595224 \n",
      "Test hamming 0.068250 \n",
      "Test accuracy 0.437584 accuracy total 652.000000 \n",
      "P5 0.259060\n",
      "Iteration 500: Training loss 0.203278 \n",
      "Train accuracy 0.720000 \n",
      "val auc macro 0.927006 micro 0.958874 \n",
      "Validation accuracy 0.681121 \n",
      "val hamming 0.053588 \n",
      "val accuracy 0.519463 total 774.000000\n",
      "Test auc macro 0.917578 micro 0.942610 \n",
      "Test f1 macro 0.479190 micro 0.603948 \n",
      "Test hamming 0.066288 \n",
      "Test accuracy 0.453020 accuracy total 675.000000 \n",
      "P5 0.260000\n",
      "Iteration 600: Training loss 0.095969 \n",
      "Train accuracy 0.720000 \n",
      "val auc macro 0.934142 micro 0.960601 \n",
      "Validation accuracy 0.677118 \n",
      "val hamming 0.050955 \n",
      "val accuracy 0.537584 total 801.000000\n",
      "Test auc macro 0.923996 micro 0.944278 \n",
      "Test f1 macro 0.516147 micro 0.612834 \n",
      "Test hamming 0.065101 \n",
      "Test accuracy 0.467114 accuracy total 696.000000 \n",
      "P5 0.260805\n",
      "Iteration 700: Training loss 0.096716 \n",
      "Train accuracy 0.730000 \n",
      "val auc macro 0.937095 micro 0.965330 \n",
      "Validation accuracy 0.691127 \n",
      "val hamming 0.050284 \n",
      "val accuracy 0.533557 total 795.000000\n",
      "Test auc macro 0.926620 micro 0.951208 \n",
      "Test f1 macro 0.528836 micro 0.629504 \n",
      "Test hamming 0.062106 \n",
      "Test accuracy 0.471812 accuracy total 703.000000 \n",
      "P5 0.263893\n",
      "Iteration 800: Training loss 0.166832 \n",
      "Train accuracy 0.730000 \n",
      "val auc macro 0.940943 micro 0.967426 \n",
      "Validation accuracy 0.691795 \n",
      "val hamming 0.050129 \n",
      "val accuracy 0.544966 total 812.000000\n",
      "Test auc macro 0.931826 micro 0.951773 \n",
      "Test f1 macro 0.554632 micro 0.634072 \n",
      "Test hamming 0.062210 \n",
      "Test accuracy 0.479866 accuracy total 715.000000 \n",
      "P5 0.263758\n",
      "Iteration 900: Training loss 0.084138 \n",
      "Train accuracy 0.680000 \n",
      "val auc macro 0.942673 micro 0.968795 \n",
      "Validation accuracy 0.701801 \n",
      "val hamming 0.049200 \n",
      "val accuracy 0.553691 total 825.000000\n",
      "Test auc macro 0.934994 micro 0.951275 \n",
      "Test f1 macro 0.565190 micro 0.639808 \n",
      "Test hamming 0.061848 \n",
      "Test accuracy 0.493289 accuracy total 735.000000 \n",
      "P5 0.263490\n",
      "Iteration 1000: Training loss 0.161614 \n",
      "Train accuracy 0.750000 \n",
      "val auc macro 0.949397 micro 0.971490 \n",
      "Validation accuracy 0.707138 \n",
      "val hamming 0.048064 \n",
      "val accuracy 0.562416 total 838.000000\n",
      "Test auc macro 0.938027 micro 0.953197 \n",
      "Test f1 macro 0.582411 micro 0.648470 \n",
      "Test hamming 0.060506 \n",
      "Test accuracy 0.496644 accuracy total 740.000000 \n",
      "P5 0.265638\n",
      "Iteration 1100: Training loss 0.069542 \n",
      "Train accuracy 0.750000 \n",
      "val auc macro 0.946756 micro 0.969547 \n",
      "Validation accuracy 0.711141 \n",
      "val hamming 0.047909 \n",
      "val accuracy 0.568456 total 847.000000\n",
      "Test auc macro 0.933662 micro 0.954653 \n",
      "Test f1 macro 0.587391 micro 0.642384 \n",
      "Test hamming 0.061332 \n",
      "Test accuracy 0.495302 accuracy total 738.000000 \n",
      "P5 0.264832\n",
      "Starting epoch 1\n",
      "Iteration 1200: Training loss 0.051613 \n",
      "Train accuracy 0.750000 \n",
      "val auc macro 0.956962 micro 0.974742 \n",
      "Validation accuracy 0.718479 \n",
      "val hamming 0.047083 \n",
      "val accuracy 0.574497 total 856.000000\n",
      "Test auc macro 0.940111 micro 0.958274 \n",
      "Test f1 macro 0.609576 micro 0.657698 \n",
      "Test hamming 0.059112 \n",
      "Test accuracy 0.506711 accuracy total 755.000000 \n",
      "P5 0.265369\n",
      "Iteration 1300: Training loss 0.104962 \n",
      "Train accuracy 0.770000 \n",
      "val auc macro 0.952594 micro 0.973821 \n",
      "Validation accuracy 0.716478 \n",
      "val hamming 0.046928 \n",
      "val accuracy 0.583221 total 869.000000\n",
      "Test auc macro 0.940829 micro 0.957251 \n",
      "Test f1 macro 0.609085 micro 0.659011 \n",
      "Test hamming 0.059783 \n",
      "Test accuracy 0.514094 accuracy total 766.000000 \n",
      "P5 0.266980\n",
      "Iteration 1400: Training loss 0.080601 \n",
      "Train accuracy 0.780000 \n",
      "val auc macro 0.952438 micro 0.973635 \n",
      "Validation accuracy 0.717145 \n",
      "val hamming 0.046464 \n",
      "val accuracy 0.591275 total 881.000000\n",
      "Test auc macro 0.942627 micro 0.957943 \n",
      "Test f1 macro 0.608029 micro 0.659739 \n",
      "Test hamming 0.059164 \n",
      "Test accuracy 0.512752 accuracy total 764.000000 \n",
      "P5 0.266846\n",
      "Iteration 1500: Training loss 0.081247 \n",
      "Train accuracy 0.780000 \n",
      "val auc macro 0.955522 micro 0.972987 \n",
      "Validation accuracy 0.712475 \n",
      "val hamming 0.047083 \n",
      "val accuracy 0.584564 total 871.000000\n",
      "Test auc macro 0.941083 micro 0.959062 \n",
      "Test f1 macro 0.616633 micro 0.654405 \n",
      "Test hamming 0.060145 \n",
      "Test accuracy 0.513423 accuracy total 765.000000 \n",
      "P5 0.266040\n",
      "Iteration 1600: Training loss 0.051658 \n",
      "Train accuracy 0.810000 \n",
      "val auc macro 0.956887 micro 0.975393 \n",
      "Validation accuracy 0.724483 \n",
      "val hamming 0.044966 \n",
      "val accuracy 0.592617 total 883.000000\n",
      "Test auc macro 0.940633 micro 0.956490 \n",
      "Test f1 macro 0.625846 micro 0.661922 \n",
      "Test hamming 0.058854 \n",
      "Test accuracy 0.519463 accuracy total 774.000000 \n",
      "P5 0.266980\n",
      "Iteration 1700: Training loss 0.069541 \n",
      "Train accuracy 0.810000 \n",
      "val auc macro 0.956455 micro 0.974712 \n",
      "Validation accuracy 0.729820 \n",
      "val hamming 0.046257 \n",
      "val accuracy 0.595302 total 887.000000\n",
      "Test auc macro 0.941338 micro 0.959860 \n",
      "Test f1 macro 0.622317 micro 0.656515 \n",
      "Test hamming 0.060558 \n",
      "Test accuracy 0.519463 accuracy total 774.000000 \n",
      "P5 0.265772\n",
      "Iteration 1800: Training loss 0.058643 \n",
      "Train accuracy 0.790000 \n",
      "val auc macro 0.958769 micro 0.975464 \n",
      "Validation accuracy 0.719813 \n",
      "val hamming 0.045638 \n",
      "val accuracy 0.595302 total 887.000000\n",
      "Test auc macro 0.941997 micro 0.956699 \n",
      "Test f1 macro 0.628424 micro 0.667449 \n",
      "Test hamming 0.058493 \n",
      "Test accuracy 0.528188 accuracy total 787.000000 \n",
      "P5 0.264966\n",
      "Iteration 1900: Training loss 0.047572 \n",
      "Train accuracy 0.790000 \n",
      "val auc macro 0.958974 micro 0.975419 \n",
      "Validation accuracy 0.722482 \n",
      "val hamming 0.045689 \n",
      "val accuracy 0.588591 total 877.000000\n",
      "Test auc macro 0.941169 micro 0.957796 \n",
      "Test f1 macro 0.635278 micro 0.670979 \n",
      "Test hamming 0.057770 \n",
      "Test accuracy 0.526846 accuracy total 785.000000 \n",
      "P5 0.265503\n",
      "Iteration 2000: Training loss 0.045642 \n",
      "Train accuracy 0.820000 \n",
      "val auc macro 0.961104 micro 0.976824 \n",
      "Validation accuracy 0.729820 \n",
      "val hamming 0.045534 \n",
      "val accuracy 0.596644 total 889.000000\n",
      "Test auc macro 0.940978 micro 0.956571 \n",
      "Test f1 macro 0.628189 micro 0.668426 \n",
      "Test hamming 0.058389 \n",
      "Test accuracy 0.531544 accuracy total 792.000000 \n",
      "P5 0.266174\n",
      "Iteration 2100: Training loss 0.103093 \n",
      "Train accuracy 0.790000 \n",
      "val auc macro 0.960689 micro 0.977220 \n",
      "Validation accuracy 0.721815 \n",
      "val hamming 0.045328 \n",
      "val accuracy 0.602013 total 897.000000\n",
      "Test auc macro 0.944436 micro 0.958981 \n",
      "Test f1 macro 0.629527 micro 0.675226 \n",
      "Test hamming 0.057460 \n",
      "Test accuracy 0.534899 accuracy total 797.000000 \n",
      "P5 0.265906\n",
      "Iteration 2200: Training loss 0.076501 \n",
      "Train accuracy 0.790000 \n",
      "val auc macro 0.960345 micro 0.976831 \n",
      "Validation accuracy 0.748499 \n",
      "val hamming 0.044812 \n",
      "val accuracy 0.603356 total 899.000000\n",
      "Test auc macro 0.943217 micro 0.956322 \n",
      "Test f1 macro 0.636850 micro 0.670346 \n",
      "Test hamming 0.058596 \n",
      "Test accuracy 0.540268 accuracy total 805.000000 \n",
      "P5 0.265235\n",
      "Iteration 2300: Training loss 0.152317 \n",
      "Train accuracy 0.800000 \n",
      "val auc macro 0.962401 micro 0.977446 \n",
      "Validation accuracy 0.739159 \n",
      "val hamming 0.044450 \n",
      "val accuracy 0.603356 total 899.000000\n",
      "Test auc macro 0.943745 micro 0.954401 \n",
      "Test f1 macro 0.644733 micro 0.676221 \n",
      "Test hamming 0.057150 \n",
      "Test accuracy 0.534899 accuracy total 797.000000 \n",
      "P5 0.265101\n",
      "Starting epoch 2\n",
      "Iteration 2400: Training loss 0.059364 \n",
      "Train accuracy 0.790000 \n",
      "val auc macro 0.960741 micro 0.976454 \n",
      "Validation accuracy 0.721147 \n",
      "val hamming 0.045379 \n",
      "val accuracy 0.604698 total 901.000000\n",
      "Test auc macro 0.944274 micro 0.957124 \n",
      "Test f1 macro 0.624993 micro 0.669390 \n",
      "Test hamming 0.058493 \n",
      "Test accuracy 0.530872 accuracy total 791.000000 \n",
      "P5 0.265503\n",
      "Iteration 2500: Training loss 0.026082 \n",
      "Train accuracy 0.840000 \n",
      "val auc macro 0.963331 micro 0.976829 \n",
      "Validation accuracy 0.749166 \n",
      "val hamming 0.046773 \n",
      "val accuracy 0.606711 total 904.000000\n",
      "Test auc macro 0.944235 micro 0.958537 \n",
      "Test f1 macro 0.629482 micro 0.666473 \n",
      "Test hamming 0.059422 \n",
      "Test accuracy 0.538255 accuracy total 802.000000 \n",
      "P5 0.265101\n",
      "Iteration 2600: Training loss 0.019858 \n",
      "Train accuracy 0.840000 \n",
      "val auc macro 0.962538 micro 0.977529 \n",
      "Validation accuracy 0.735157 \n",
      "val hamming 0.045018 \n",
      "val accuracy 0.608725 total 907.000000\n",
      "Test auc macro 0.946777 micro 0.957949 \n",
      "Test f1 macro 0.632457 micro 0.677111 \n",
      "Test hamming 0.057460 \n",
      "Test accuracy 0.542282 accuracy total 808.000000 \n",
      "P5 0.264966\n",
      "Iteration 2700: Training loss 0.067348 \n",
      "Train accuracy 0.790000 \n",
      "val auc macro 0.961809 micro 0.976596 \n",
      "Validation accuracy 0.729820 \n",
      "val hamming 0.044915 \n",
      "val accuracy 0.613423 total 914.000000\n",
      "Test auc macro 0.943549 micro 0.959577 \n",
      "Test f1 macro 0.644911 micro 0.674358 \n",
      "Test hamming 0.058286 \n",
      "Test accuracy 0.543624 accuracy total 810.000000 \n",
      "P5 0.265503\n",
      "Iteration 2800: Training loss 0.057991 \n",
      "Train accuracy 0.830000 \n",
      "val auc macro 0.958957 micro 0.975665 \n",
      "Validation accuracy 0.741161 \n",
      "val hamming 0.044760 \n",
      "val accuracy 0.614094 total 915.000000\n",
      "Test auc macro 0.944004 micro 0.955995 \n",
      "Test f1 macro 0.632365 micro 0.670500 \n",
      "Test hamming 0.059164 \n",
      "Test accuracy 0.537584 accuracy total 801.000000 \n",
      "P5 0.263356\n",
      "Iteration 2900: Training loss 0.091125 \n",
      "Train accuracy 0.840000 \n",
      "val auc macro 0.959870 micro 0.976938 \n",
      "Validation accuracy 0.735824 \n",
      "val hamming 0.045483 \n",
      "val accuracy 0.608054 total 906.000000\n",
      "Test auc macro 0.945385 micro 0.958684 \n",
      "Test f1 macro 0.627371 micro 0.673210 \n",
      "Test hamming 0.058441 \n",
      "Test accuracy 0.538926 accuracy total 803.000000 \n",
      "P5 0.266309\n",
      "Iteration 3000: Training loss 0.036827 \n",
      "Train accuracy 0.830000 \n",
      "val auc macro 0.959196 micro 0.974190 \n",
      "Validation accuracy 0.730487 \n",
      "val hamming 0.046257 \n",
      "val accuracy 0.607383 total 905.000000\n",
      "Test auc macro 0.942266 micro 0.958041 \n",
      "Test f1 macro 0.638012 micro 0.670308 \n",
      "Test hamming 0.059215 \n",
      "Test accuracy 0.543624 accuracy total 810.000000 \n",
      "P5 0.264295\n",
      "Iteration 3100: Training loss 0.124043 \n",
      "Train accuracy 0.840000 \n",
      "val auc macro 0.960950 micro 0.975816 \n",
      "Validation accuracy 0.735157 \n",
      "val hamming 0.044450 \n",
      "val accuracy 0.610738 total 910.000000\n",
      "Test auc macro 0.941782 micro 0.956772 \n",
      "Test f1 macro 0.641134 micro 0.674601 \n",
      "Test hamming 0.057873 \n",
      "Test accuracy 0.543624 accuracy total 810.000000 \n",
      "P5 0.262685\n",
      "Iteration 3200: Training loss 0.094353 \n",
      "Train accuracy 0.800000 \n",
      "val auc macro 0.962537 micro 0.977138 \n",
      "Validation accuracy 0.722482 \n",
      "val hamming 0.045070 \n",
      "val accuracy 0.606040 total 903.000000\n",
      "Test auc macro 0.946137 micro 0.958766 \n",
      "Test f1 macro 0.628913 micro 0.673623 \n",
      "Test hamming 0.058131 \n",
      "Test accuracy 0.540268 accuracy total 805.000000 \n",
      "P5 0.266174\n",
      "Iteration 3300: Training loss 0.062047 \n",
      "Train accuracy 0.830000 \n",
      "val auc macro 0.961354 micro 0.975456 \n",
      "Validation accuracy 0.739159 \n",
      "val hamming 0.044244 \n",
      "val accuracy 0.614094 total 915.000000\n",
      "Test auc macro 0.944803 micro 0.958329 \n",
      "Test f1 macro 0.642298 micro 0.678872 \n",
      "Test hamming 0.057047 \n",
      "Test accuracy 0.546980 accuracy total 815.000000 \n",
      "P5 0.265235\n",
      "Iteration 3400: Training loss 0.046190 \n",
      "Train accuracy 0.820000 \n",
      "val auc macro 0.963245 micro 0.976113 \n",
      "Validation accuracy 0.733155 \n",
      "val hamming 0.043882 \n",
      "val accuracy 0.618121 total 921.000000\n",
      "Test auc macro 0.943391 micro 0.956063 \n",
      "Test f1 macro 0.640816 micro 0.678221 \n",
      "Test hamming 0.057512 \n",
      "Test accuracy 0.546309 accuracy total 814.000000 \n",
      "P5 0.263758\n",
      "Iteration 3500: Training loss 0.098174 \n",
      "Train accuracy 0.820000 \n",
      "val auc macro 0.961312 micro 0.975961 \n",
      "Validation accuracy 0.732488 \n",
      "val hamming 0.045328 \n",
      "val accuracy 0.607383 total 905.000000\n",
      "Test auc macro 0.942473 micro 0.954901 \n",
      "Test f1 macro 0.636417 micro 0.676932 \n",
      "Test hamming 0.057408 \n",
      "Test accuracy 0.542953 accuracy total 809.000000 \n",
      "P5 0.263624\n",
      "Starting epoch 3\n",
      "Iteration 3600: Training loss 0.017451 \n",
      "Train accuracy 0.850000 \n",
      "val auc macro 0.963486 micro 0.978140 \n",
      "Validation accuracy 0.737825 \n",
      "val hamming 0.045276 \n",
      "val accuracy 0.608054 total 906.000000\n",
      "Test auc macro 0.944574 micro 0.958058 \n",
      "Test f1 macro 0.651111 micro 0.680209 \n",
      "Test hamming 0.056892 \n",
      "Test accuracy 0.546980 accuracy total 815.000000 \n",
      "P5 0.264698\n",
      "Iteration 3700: Training loss 0.015303 \n",
      "Train accuracy 0.840000 \n",
      "val auc macro 0.962221 micro 0.977554 \n",
      "Validation accuracy 0.747832 \n",
      "val hamming 0.044605 \n",
      "val accuracy 0.619463 total 923.000000\n",
      "Test auc macro 0.942533 micro 0.958011 \n",
      "Test f1 macro 0.652242 micro 0.680827 \n",
      "Test hamming 0.057408 \n",
      "Test accuracy 0.553020 accuracy total 824.000000 \n",
      "P5 0.266040\n",
      "Iteration 3800: Training loss 0.036987 \n",
      "Train accuracy 0.850000 \n",
      "val auc macro 0.960791 micro 0.975794 \n",
      "Validation accuracy 0.737825 \n",
      "val hamming 0.045947 \n",
      "val accuracy 0.611409 total 911.000000\n",
      "Test auc macro 0.939445 micro 0.955762 \n",
      "Test f1 macro 0.643767 micro 0.676835 \n",
      "Test hamming 0.057976 \n",
      "Test accuracy 0.548322 accuracy total 817.000000 \n",
      "P5 0.263356\n",
      "Iteration 3900: Training loss 0.015853 \n",
      "Train accuracy 0.800000 \n",
      "val auc macro 0.959248 micro 0.972329 \n",
      "Validation accuracy 0.716478 \n",
      "val hamming 0.045896 \n",
      "val accuracy 0.607383 total 905.000000\n",
      "Test auc macro 0.940455 micro 0.955573 \n",
      "Test f1 macro 0.627469 micro 0.670697 \n",
      "Test hamming 0.059060 \n",
      "Test accuracy 0.536913 accuracy total 800.000000 \n",
      "P5 0.263624\n",
      "Iteration 4000: Training loss 0.019894 \n",
      "Train accuracy 0.810000 \n",
      "val auc macro 0.960882 micro 0.973974 \n",
      "Validation accuracy 0.732488 \n",
      "val hamming 0.045483 \n",
      "val accuracy 0.606711 total 904.000000\n",
      "Test auc macro 0.937307 micro 0.955178 \n",
      "Test f1 macro 0.640195 micro 0.675520 \n",
      "Test hamming 0.058028 \n",
      "Test accuracy 0.542953 accuracy total 809.000000 \n",
      "P5 0.263221\n",
      "Iteration 4100: Training loss 0.054531 \n",
      "Train accuracy 0.810000 \n",
      "val auc macro 0.959602 micro 0.973889 \n",
      "Validation accuracy 0.729820 \n",
      "val hamming 0.045018 \n",
      "val accuracy 0.616779 total 919.000000\n",
      "Test auc macro 0.938638 micro 0.953439 \n",
      "Test f1 macro 0.639498 micro 0.675093 \n",
      "Test hamming 0.058389 \n",
      "Test accuracy 0.544966 accuracy total 812.000000 \n",
      "P5 0.263221\n",
      "Iteration 4200: Training loss 0.012140 \n",
      "Train accuracy 0.850000 \n",
      "val auc macro 0.958961 micro 0.974125 \n",
      "Validation accuracy 0.745163 \n",
      "val hamming 0.044605 \n",
      "val accuracy 0.616107 total 918.000000\n",
      "Test auc macro 0.939032 micro 0.953140 \n",
      "Test f1 macro 0.640802 micro 0.671478 \n",
      "Test hamming 0.058751 \n",
      "Test accuracy 0.540268 accuracy total 805.000000 \n",
      "P5 0.262953\n",
      "Iteration 4300: Training loss 0.077526 \n",
      "Train accuracy 0.860000 \n",
      "val auc macro 0.956689 micro 0.973379 \n",
      "Validation accuracy 0.730487 \n",
      "val hamming 0.046051 \n",
      "val accuracy 0.607383 total 905.000000\n",
      "Test auc macro 0.937058 micro 0.948479 \n",
      "Test f1 macro 0.640709 micro 0.674734 \n",
      "Test hamming 0.058286 \n",
      "Test accuracy 0.542282 accuracy total 808.000000 \n",
      "P5 0.258255\n",
      "Iteration 4400: Training loss 0.050387 \n",
      "Train accuracy 0.840000 \n",
      "val auc macro 0.958517 micro 0.975112 \n",
      "Validation accuracy 0.727819 \n",
      "val hamming 0.045638 \n",
      "val accuracy 0.609396 total 908.000000\n",
      "Test auc macro 0.940292 micro 0.957168 \n",
      "Test f1 macro 0.634036 micro 0.674532 \n",
      "Test hamming 0.058389 \n",
      "Test accuracy 0.542953 accuracy total 809.000000 \n",
      "P5 0.264161\n",
      "Iteration 4500: Training loss 0.026032 \n",
      "Train accuracy 0.860000 \n",
      "val auc macro 0.955329 micro 0.972883 \n",
      "Validation accuracy 0.734490 \n",
      "val hamming 0.046722 \n",
      "val accuracy 0.605369 total 902.000000\n",
      "Test auc macro 0.938583 micro 0.956749 \n",
      "Test f1 macro 0.651151 micro 0.673932 \n",
      "Test hamming 0.058699 \n",
      "Test accuracy 0.547651 accuracy total 816.000000 \n",
      "P5 0.263356\n",
      "Iteration 4600: Training loss 0.006226 \n",
      "Train accuracy 0.850000 \n",
      "val auc macro 0.961477 micro 0.976006 \n",
      "Validation accuracy 0.729820 \n",
      "val hamming 0.044399 \n",
      "val accuracy 0.611409 total 911.000000\n",
      "Test auc macro 0.942137 micro 0.951093 \n",
      "Test f1 macro 0.652602 micro 0.679343 \n",
      "Test hamming 0.057460 \n",
      "Test accuracy 0.549664 accuracy total 819.000000 \n",
      "P5 0.261074\n",
      "Iteration 4700: Training loss 0.018077 \n",
      "Train accuracy 0.820000 \n",
      "val auc macro 0.952039 micro 0.973379 \n",
      "Validation accuracy 0.723149 \n",
      "val hamming 0.044708 \n",
      "val accuracy 0.614094 total 915.000000\n",
      "Test auc macro 0.936892 micro 0.956280 \n",
      "Test f1 macro 0.640780 micro 0.674352 \n",
      "Test hamming 0.058338 \n",
      "Test accuracy 0.545638 accuracy total 813.000000 \n",
      "P5 0.265101\n",
      "Starting epoch 4\n",
      "Iteration 4800: Training loss 0.020578 \n",
      "Train accuracy 0.850000 \n",
      "val auc macro 0.955844 micro 0.974478 \n",
      "Validation accuracy 0.732488 \n",
      "val hamming 0.045586 \n",
      "val accuracy 0.611409 total 911.000000\n",
      "Test auc macro 0.935648 micro 0.955467 \n",
      "Test f1 macro 0.636616 micro 0.671264 \n",
      "Test hamming 0.059060 \n",
      "Test accuracy 0.539597 accuracy total 804.000000 \n",
      "P5 0.264698\n",
      "Iteration 4900: Training loss 0.039143 \n",
      "Train accuracy 0.840000 \n",
      "val auc macro 0.953018 micro 0.974350 \n",
      "Validation accuracy 0.717145 \n",
      "val hamming 0.045689 \n",
      "val accuracy 0.611409 total 911.000000\n",
      "Test auc macro 0.936260 micro 0.957075 \n",
      "Test f1 macro 0.644215 micro 0.677161 \n",
      "Test hamming 0.058234 \n",
      "Test accuracy 0.548322 accuracy total 817.000000 \n",
      "P5 0.265503\n",
      "Iteration 5000: Training loss 0.007078 \n",
      "Train accuracy 0.830000 \n",
      "val auc macro 0.950853 micro 0.972429 \n",
      "Validation accuracy 0.730487 \n",
      "val hamming 0.044605 \n",
      "val accuracy 0.616779 total 919.000000\n",
      "Test auc macro 0.931089 micro 0.953635 \n",
      "Test f1 macro 0.639078 micro 0.666667 \n",
      "Test hamming 0.059938 \n",
      "Test accuracy 0.541611 accuracy total 807.000000 \n",
      "P5 0.263356\n",
      "Iteration 5100: Training loss 0.002440 \n",
      "Train accuracy 0.850000 \n",
      "val auc macro 0.955084 micro 0.974306 \n",
      "Validation accuracy 0.737825 \n",
      "val hamming 0.045844 \n",
      "val accuracy 0.615436 total 917.000000\n",
      "Test auc macro 0.933730 micro 0.953857 \n",
      "Test f1 macro 0.637861 micro 0.669550 \n",
      "Test hamming 0.059164 \n",
      "Test accuracy 0.540268 accuracy total 805.000000 \n",
      "P5 0.262013\n",
      "Iteration 5200: Training loss 0.021386 \n",
      "Train accuracy 0.840000 \n",
      "val auc macro 0.949422 micro 0.971302 \n",
      "Validation accuracy 0.727819 \n",
      "val hamming 0.045792 \n",
      "val accuracy 0.612081 total 912.000000\n",
      "Test auc macro 0.931134 micro 0.952605 \n",
      "Test f1 macro 0.636543 micro 0.665332 \n",
      "Test hamming 0.060403 \n",
      "Test accuracy 0.538255 accuracy total 802.000000 \n",
      "P5 0.262685\n",
      "Iteration 5300: Training loss 0.026628 \n",
      "Train accuracy 0.860000 \n",
      "val auc macro 0.953394 micro 0.973904 \n",
      "Validation accuracy 0.731154 \n",
      "val hamming 0.046257 \n",
      "val accuracy 0.610738 total 910.000000\n",
      "Test auc macro 0.936038 micro 0.955405 \n",
      "Test f1 macro 0.638042 micro 0.667809 \n",
      "Test hamming 0.060041 \n",
      "Test accuracy 0.545638 accuracy total 813.000000 \n",
      "P5 0.264027\n",
      "Iteration 5400: Training loss 0.047652 \n",
      "Train accuracy 0.870000 \n",
      "val auc macro 0.950700 micro 0.970045 \n",
      "Validation accuracy 0.721815 \n",
      "val hamming 0.046051 \n",
      "val accuracy 0.615436 total 917.000000\n",
      "Test auc macro 0.930099 micro 0.948804 \n",
      "Test f1 macro 0.645764 micro 0.667237 \n",
      "Test hamming 0.060196 \n",
      "Test accuracy 0.542282 accuracy total 808.000000 \n",
      "P5 0.261477\n",
      "Iteration 5500: Training loss 0.021979 \n",
      "Train accuracy 0.850000 \n",
      "val auc macro 0.947874 micro 0.971298 \n",
      "Validation accuracy 0.734490 \n",
      "val hamming 0.045999 \n",
      "val accuracy 0.618121 total 921.000000\n",
      "Test auc macro 0.932662 micro 0.953218 \n",
      "Test f1 macro 0.649437 micro 0.671599 \n",
      "Test hamming 0.059577 \n",
      "Test accuracy 0.544295 accuracy total 811.000000 \n",
      "P5 0.262685\n",
      "Iteration 5600: Training loss 0.032821 \n",
      "Train accuracy 0.810000 \n",
      "val auc macro 0.947187 micro 0.968610 \n",
      "Validation accuracy 0.721815 \n",
      "val hamming 0.046877 \n",
      "val accuracy 0.608725 total 907.000000\n",
      "Test auc macro 0.929669 micro 0.949048 \n",
      "Test f1 macro 0.637767 micro 0.665907 \n",
      "Test hamming 0.060558 \n",
      "Test accuracy 0.544295 accuracy total 811.000000 \n",
      "P5 0.261879\n",
      "Iteration 5700: Training loss 0.029537 \n",
      "Train accuracy 0.830000 \n",
      "val auc macro 0.952523 micro 0.972285 \n",
      "Validation accuracy 0.724483 \n",
      "val hamming 0.046051 \n",
      "val accuracy 0.614094 total 915.000000\n",
      "Test auc macro 0.934683 micro 0.949216 \n",
      "Test f1 macro 0.638531 micro 0.676201 \n",
      "Test hamming 0.058441 \n",
      "Test accuracy 0.548993 accuracy total 818.000000 \n",
      "P5 0.259060\n",
      "Iteration 5800: Training loss 0.032153 \n",
      "Train accuracy 0.890000 \n",
      "val auc macro 0.955384 micro 0.972562 \n",
      "Validation accuracy 0.737158 \n",
      "val hamming 0.045792 \n",
      "val accuracy 0.616107 total 918.000000\n",
      "Test auc macro 0.935466 micro 0.953076 \n",
      "Test f1 macro 0.641942 micro 0.671795 \n",
      "Test hamming 0.059473 \n",
      "Test accuracy 0.546980 accuracy total 815.000000 \n",
      "P5 0.260940\n",
      "Iteration 5900: Training loss 0.019045 \n",
      "Train accuracy 0.910000 \n",
      "val auc macro 0.955914 micro 0.973279 \n",
      "Validation accuracy 0.742495 \n",
      "val hamming 0.045328 \n",
      "val accuracy 0.612752 total 913.000000\n",
      "Test auc macro 0.935952 micro 0.950379 \n",
      "Test f1 macro 0.632884 micro 0.664956 \n",
      "Test hamming 0.060661 \n",
      "Test accuracy 0.542953 accuracy total 809.000000 \n",
      "P5 0.261611\n",
      "Starting epoch 5\n",
      "Iteration 6000: Training loss 0.050358 \n",
      "Train accuracy 0.860000 \n",
      "val auc macro 0.950824 micro 0.972345 \n",
      "Validation accuracy 0.729153 \n",
      "val hamming 0.045689 \n",
      "val accuracy 0.612752 total 913.000000\n",
      "Test auc macro 0.935154 micro 0.952192 \n",
      "Test f1 macro 0.634450 micro 0.671056 \n",
      "Test hamming 0.059319 \n",
      "Test accuracy 0.546309 accuracy total 814.000000 \n",
      "P5 0.262282\n",
      "Iteration 6100: Training loss 0.057108 \n",
      "Train accuracy 0.890000 \n",
      "val auc macro 0.948922 micro 0.970817 \n",
      "Validation accuracy 0.738492 \n",
      "val hamming 0.046722 \n",
      "val accuracy 0.610738 total 910.000000\n",
      "Test auc macro 0.934439 micro 0.951135 \n",
      "Test f1 macro 0.627046 micro 0.664945 \n",
      "Test hamming 0.060299 \n",
      "Test accuracy 0.540268 accuracy total 805.000000 \n",
      "P5 0.262953\n",
      "Iteration 6200: Training loss 0.005356 \n",
      "Train accuracy 0.830000 \n",
      "val auc macro 0.945432 micro 0.969375 \n",
      "Validation accuracy 0.737158 \n",
      "val hamming 0.047238 \n",
      "val accuracy 0.610738 total 910.000000\n",
      "Test auc macro 0.936023 micro 0.951108 \n",
      "Test f1 macro 0.633603 micro 0.670829 \n",
      "Test hamming 0.059886 \n",
      "Test accuracy 0.547651 accuracy total 816.000000 \n",
      "P5 0.261342\n",
      "Iteration 6300: Training loss 0.003689 \n",
      "Train accuracy 0.860000 \n",
      "val auc macro 0.946261 micro 0.970300 \n",
      "Validation accuracy 0.727819 \n",
      "val hamming 0.046412 \n",
      "val accuracy 0.608725 total 907.000000\n",
      "Test auc macro 0.932096 micro 0.952382 \n",
      "Test f1 macro 0.630609 micro 0.663431 \n",
      "Test hamming 0.060867 \n",
      "Test accuracy 0.542953 accuracy total 809.000000 \n",
      "P5 0.261208\n",
      "Iteration 6400: Training loss 0.001003 \n",
      "Train accuracy 0.870000 \n",
      "val auc macro 0.942043 micro 0.969410 \n",
      "Validation accuracy 0.725150 \n",
      "val hamming 0.046825 \n",
      "val accuracy 0.606711 total 904.000000\n",
      "Test auc macro 0.925008 micro 0.951311 \n",
      "Test f1 macro 0.637500 micro 0.664573 \n",
      "Test hamming 0.060661 \n",
      "Test accuracy 0.539597 accuracy total 804.000000 \n",
      "P5 0.262416\n",
      "Iteration 6500: Training loss 0.022874 \n",
      "Train accuracy 0.840000 \n",
      "val auc macro 0.944306 micro 0.970695 \n",
      "Validation accuracy 0.731821 \n",
      "val hamming 0.046154 \n",
      "val accuracy 0.611409 total 911.000000\n",
      "Test auc macro 0.926269 micro 0.951149 \n",
      "Test f1 macro 0.637301 micro 0.666477 \n",
      "Test hamming 0.060454 \n",
      "Test accuracy 0.542953 accuracy total 809.000000 \n",
      "P5 0.261074\n",
      "Iteration 6600: Training loss 0.025172 \n",
      "Train accuracy 0.830000 \n",
      "val auc macro 0.951895 micro 0.972106 \n",
      "Validation accuracy 0.733823 \n",
      "val hamming 0.046154 \n",
      "val accuracy 0.616107 total 918.000000\n",
      "Test auc macro 0.933475 micro 0.951764 \n",
      "Test f1 macro 0.634007 micro 0.662890 \n",
      "Test hamming 0.061435 \n",
      "Test accuracy 0.544295 accuracy total 811.000000 \n",
      "P5 0.261074\n",
      "Iteration 6700: Training loss 0.017140 \n",
      "Train accuracy 0.880000 \n",
      "val auc macro 0.949143 micro 0.970785 \n",
      "Validation accuracy 0.732488 \n",
      "val hamming 0.046773 \n",
      "val accuracy 0.611409 total 911.000000\n",
      "Test auc macro 0.926570 micro 0.950685 \n",
      "Test f1 macro 0.630833 micro 0.664202 \n",
      "Test hamming 0.060971 \n",
      "Test accuracy 0.545638 accuracy total 813.000000 \n",
      "P5 0.261342\n",
      "Iteration 6800: Training loss 0.011881 \n",
      "Train accuracy 0.850000 \n",
      "val auc macro 0.947296 micro 0.970146 \n",
      "Validation accuracy 0.735824 \n",
      "val hamming 0.045947 \n",
      "val accuracy 0.613423 total 914.000000\n",
      "Test auc macro 0.928746 micro 0.948815 \n",
      "Test f1 macro 0.639345 micro 0.669139 \n",
      "Test hamming 0.059886 \n",
      "Test accuracy 0.549664 accuracy total 819.000000 \n",
      "P5 0.259060\n",
      "Iteration 6900: Training loss 0.023783 \n",
      "Train accuracy 0.880000 \n",
      "val auc macro 0.953972 micro 0.974196 \n",
      "Validation accuracy 0.738492 \n",
      "val hamming 0.046773 \n",
      "val accuracy 0.609396 total 908.000000\n",
      "Test auc macro 0.932552 micro 0.950530 \n",
      "Test f1 macro 0.637520 micro 0.667617 \n",
      "Test hamming 0.060196 \n",
      "Test accuracy 0.544966 accuracy total 812.000000 \n",
      "P5 0.260403\n",
      "Iteration 7000: Training loss 0.013888 \n",
      "Train accuracy 0.850000 \n",
      "val auc macro 0.949872 micro 0.971527 \n",
      "Validation accuracy 0.735157 \n",
      "val hamming 0.045896 \n",
      "val accuracy 0.617450 total 920.000000\n",
      "Test auc macro 0.930906 micro 0.949464 \n",
      "Test f1 macro 0.648891 micro 0.673312 \n",
      "Test hamming 0.059215 \n",
      "Test accuracy 0.552349 accuracy total 823.000000 \n",
      "P5 0.260805\n",
      "Iteration 7100: Training loss 0.002492 \n",
      "Train accuracy 0.860000 \n",
      "val auc macro 0.947593 micro 0.970557 \n",
      "Validation accuracy 0.735157 \n",
      "val hamming 0.045947 \n",
      "val accuracy 0.612081 total 912.000000\n",
      "Test auc macro 0.928374 micro 0.949588 \n",
      "Test f1 macro 0.648251 micro 0.668194 \n",
      "Test hamming 0.059835 \n",
      "Test accuracy 0.544966 accuracy total 812.000000 \n",
      "P5 0.260940\n",
      "Starting epoch 6\n",
      "Iteration 7200: Training loss 0.014360 \n",
      "Train accuracy 0.880000 \n",
      "val auc macro 0.951691 micro 0.970325 \n",
      "Validation accuracy 0.713809 \n",
      "val hamming 0.046309 \n",
      "val accuracy 0.604027 total 900.000000\n",
      "Test auc macro 0.926245 micro 0.944280 \n",
      "Test f1 macro 0.636261 micro 0.668001 \n",
      "Test hamming 0.059938 \n",
      "Test accuracy 0.541611 accuracy total 807.000000 \n",
      "P5 0.258389\n",
      "Iteration 7300: Training loss 0.001724 \n",
      "Train accuracy 0.860000 \n",
      "val auc macro 0.948802 micro 0.969748 \n",
      "Validation accuracy 0.718479 \n",
      "val hamming 0.046980 \n",
      "val accuracy 0.608725 total 907.000000\n",
      "Test auc macro 0.927437 micro 0.947594 \n",
      "Test f1 macro 0.637313 micro 0.670655 \n",
      "Test hamming 0.059680 \n",
      "Test accuracy 0.546309 accuracy total 814.000000 \n",
      "P5 0.259195\n",
      "Iteration 7400: Training loss 0.012311 \n",
      "Train accuracy 0.850000 \n",
      "val auc macro 0.952368 micro 0.971733 \n",
      "Validation accuracy 0.733823 \n",
      "val hamming 0.046928 \n",
      "val accuracy 0.603356 total 899.000000\n",
      "Test auc macro 0.932531 micro 0.947204 \n",
      "Test f1 macro 0.642561 micro 0.664198 \n",
      "Test hamming 0.060867 \n",
      "Test accuracy 0.540268 accuracy total 805.000000 \n",
      "P5 0.259060\n",
      "Iteration 7500: Training loss 0.008082 \n",
      "Train accuracy 0.840000 \n",
      "val auc macro 0.951037 micro 0.971005 \n",
      "Validation accuracy 0.738492 \n",
      "val hamming 0.047186 \n",
      "val accuracy 0.603356 total 899.000000\n",
      "Test auc macro 0.933066 micro 0.948907 \n",
      "Test f1 macro 0.648576 micro 0.668950 \n",
      "Test hamming 0.059886 \n",
      "Test accuracy 0.538926 accuracy total 803.000000 \n",
      "P5 0.259060\n",
      "Iteration 7600: Training loss 0.051950 \n",
      "Train accuracy 0.860000 \n",
      "val auc macro 0.947042 micro 0.968919 \n",
      "Validation accuracy 0.727151 \n",
      "val hamming 0.046980 \n",
      "val accuracy 0.606040 total 903.000000\n",
      "Test auc macro 0.929538 micro 0.949258 \n",
      "Test f1 macro 0.636441 micro 0.663818 \n",
      "Test hamming 0.060919 \n",
      "Test accuracy 0.540940 accuracy total 806.000000 \n",
      "P5 0.260134\n",
      "Iteration 7700: Training loss 0.030374 \n",
      "Train accuracy 0.830000 \n",
      "val auc macro 0.950484 micro 0.970969 \n",
      "Validation accuracy 0.738492 \n",
      "val hamming 0.046515 \n",
      "val accuracy 0.608054 total 906.000000\n",
      "Test auc macro 0.931606 micro 0.947364 \n",
      "Test f1 macro 0.639436 micro 0.667425 \n",
      "Test hamming 0.060351 \n",
      "Test accuracy 0.540268 accuracy total 805.000000 \n",
      "P5 0.259463\n",
      "Iteration 7800: Training loss 0.038234 \n",
      "Train accuracy 0.850000 \n",
      "val auc macro 0.948925 micro 0.970487 \n",
      "Validation accuracy 0.737158 \n",
      "val hamming 0.045689 \n",
      "val accuracy 0.616779 total 919.000000\n",
      "Test auc macro 0.930013 micro 0.948424 \n",
      "Test f1 macro 0.633155 micro 0.663639 \n",
      "Test hamming 0.061177 \n",
      "Test accuracy 0.540940 accuracy total 806.000000 \n",
      "P5 0.259195\n",
      "Iteration 7900: Training loss 0.005369 \n",
      "Train accuracy 0.860000 \n",
      "val auc macro 0.949160 micro 0.969984 \n",
      "Validation accuracy 0.730487 \n",
      "val hamming 0.047496 \n",
      "val accuracy 0.607383 total 905.000000\n",
      "Test auc macro 0.931292 micro 0.946485 \n",
      "Test f1 macro 0.643366 micro 0.662297 \n",
      "Test hamming 0.061177 \n",
      "Test accuracy 0.538255 accuracy total 802.000000 \n",
      "P5 0.257718\n",
      "Iteration 8000: Training loss 0.057782 \n",
      "Train accuracy 0.850000 \n",
      "val auc macro 0.945776 micro 0.967756 \n",
      "Validation accuracy 0.731821 \n",
      "val hamming 0.046412 \n",
      "val accuracy 0.608054 total 906.000000\n",
      "Test auc macro 0.929528 micro 0.943190 \n",
      "Test f1 macro 0.643598 micro 0.670264 \n",
      "Test hamming 0.059938 \n",
      "Test accuracy 0.545638 accuracy total 813.000000 \n",
      "P5 0.256913\n",
      "Iteration 8100: Training loss 0.003087 \n",
      "Train accuracy 0.860000 \n",
      "val auc macro 0.946664 micro 0.969501 \n",
      "Validation accuracy 0.726484 \n",
      "val hamming 0.046257 \n",
      "val accuracy 0.611409 total 911.000000\n",
      "Test auc macro 0.925574 micro 0.946243 \n",
      "Test f1 macro 0.636233 micro 0.667426 \n",
      "Test hamming 0.060299 \n",
      "Test accuracy 0.544295 accuracy total 811.000000 \n",
      "P5 0.259060\n",
      "Iteration 8200: Training loss 0.017877 \n",
      "Train accuracy 0.860000 \n",
      "val auc macro 0.948650 micro 0.970253 \n",
      "Validation accuracy 0.723149 \n",
      "val hamming 0.047496 \n",
      "val accuracy 0.608054 total 906.000000\n",
      "Test auc macro 0.926466 micro 0.945000 \n",
      "Test f1 macro 0.627685 micro 0.662685 \n",
      "Test hamming 0.061229 \n",
      "Test accuracy 0.540268 accuracy total 805.000000 \n",
      "P5 0.258658\n",
      "Iteration 8300: Training loss 0.002760 \n",
      "Train accuracy 0.880000 \n",
      "val auc macro 0.950306 micro 0.968514 \n",
      "Validation accuracy 0.729153 \n",
      "val hamming 0.047135 \n",
      "val accuracy 0.606711 total 904.000000\n",
      "Test auc macro 0.925618 micro 0.942966 \n",
      "Test f1 macro 0.638193 micro 0.664573 \n",
      "Test hamming 0.060661 \n",
      "Test accuracy 0.544966 accuracy total 812.000000 \n",
      "P5 0.258121\n",
      "Starting epoch 7\n",
      "Iteration 8400: Training loss 0.011100 \n",
      "Train accuracy 0.850000 \n",
      "val auc macro 0.947486 micro 0.969247 \n",
      "Validation accuracy 0.723149 \n",
      "val hamming 0.046464 \n",
      "val accuracy 0.608725 total 907.000000\n",
      "Test auc macro 0.924374 micro 0.943830 \n",
      "Test f1 macro 0.640908 micro 0.668944 \n",
      "Test hamming 0.060041 \n",
      "Test accuracy 0.544295 accuracy total 811.000000 \n",
      "P5 0.260000\n",
      "Iteration 8500: Training loss 0.000816 \n",
      "Train accuracy 0.880000 \n",
      "val auc macro 0.948681 micro 0.969201 \n",
      "Validation accuracy 0.723149 \n",
      "val hamming 0.047393 \n",
      "val accuracy 0.608725 total 907.000000\n",
      "Test auc macro 0.922296 micro 0.947437 \n",
      "Test f1 macro 0.651197 micro 0.670832 \n",
      "Test hamming 0.059835 \n",
      "Test accuracy 0.546309 accuracy total 814.000000 \n",
      "P5 0.261611\n",
      "Iteration 8600: Training loss 0.005636 \n",
      "Train accuracy 0.830000 \n",
      "val auc macro 0.941253 micro 0.967366 \n",
      "Validation accuracy 0.721147 \n",
      "val hamming 0.047186 \n",
      "val accuracy 0.610067 total 909.000000\n",
      "Test auc macro 0.920655 micro 0.945047 \n",
      "Test f1 macro 0.650721 micro 0.672159 \n",
      "Test hamming 0.059577 \n",
      "Test accuracy 0.550336 accuracy total 820.000000 \n",
      "P5 0.259463\n",
      "Iteration 8700: Training loss 0.018490 \n",
      "Train accuracy 0.810000 \n",
      "val auc macro 0.943476 micro 0.966535 \n",
      "Validation accuracy 0.729820 \n",
      "val hamming 0.047548 \n",
      "val accuracy 0.604698 total 901.000000\n",
      "Test auc macro 0.921209 micro 0.943993 \n",
      "Test f1 macro 0.653999 micro 0.666478 \n",
      "Test hamming 0.060712 \n",
      "Test accuracy 0.543624 accuracy total 810.000000 \n",
      "P5 0.259732\n",
      "Iteration 8800: Training loss 0.003658 \n",
      "Train accuracy 0.830000 \n",
      "val auc macro 0.947506 micro 0.968528 \n",
      "Validation accuracy 0.724483 \n",
      "val hamming 0.046412 \n",
      "val accuracy 0.611409 total 911.000000\n",
      "Test auc macro 0.924526 micro 0.945556 \n",
      "Test f1 macro 0.642548 micro 0.663268 \n",
      "Test hamming 0.061384 \n",
      "Test accuracy 0.545638 accuracy total 813.000000 \n",
      "P5 0.259597\n",
      "Iteration 8900: Training loss 0.006395 \n",
      "Train accuracy 0.830000 \n",
      "val auc macro 0.944126 micro 0.967266 \n",
      "Validation accuracy 0.712475 \n",
      "val hamming 0.047651 \n",
      "val accuracy 0.614094 total 915.000000\n",
      "Test auc macro 0.922025 micro 0.944212 \n",
      "Test f1 macro 0.648269 micro 0.669496 \n",
      "Test hamming 0.060299 \n",
      "Test accuracy 0.547651 accuracy total 816.000000 \n",
      "P5 0.259597\n",
      "Iteration 9000: Training loss 0.057772 \n",
      "Train accuracy 0.830000 \n",
      "val auc macro 0.950254 micro 0.971035 \n",
      "Validation accuracy 0.723149 \n",
      "val hamming 0.047341 \n",
      "val accuracy 0.603356 total 899.000000\n",
      "Test auc macro 0.922891 micro 0.946947 \n",
      "Test f1 macro 0.639628 micro 0.664018 \n",
      "Test hamming 0.061125 \n",
      "Test accuracy 0.541611 accuracy total 807.000000 \n",
      "P5 0.261074\n",
      "Iteration 9100: Training loss 0.012158 \n",
      "Train accuracy 0.860000 \n",
      "val auc macro 0.947515 micro 0.968868 \n",
      "Validation accuracy 0.733155 \n",
      "val hamming 0.048012 \n",
      "val accuracy 0.605369 total 902.000000\n",
      "Test auc macro 0.921402 micro 0.944418 \n",
      "Test f1 macro 0.643357 micro 0.666289 \n",
      "Test hamming 0.060764 \n",
      "Test accuracy 0.545638 accuracy total 813.000000 \n",
      "P5 0.258523\n",
      "Iteration 9200: Training loss 0.068612 \n",
      "Train accuracy 0.870000 \n",
      "val auc macro 0.946579 micro 0.969577 \n",
      "Validation accuracy 0.732488 \n",
      "val hamming 0.047083 \n",
      "val accuracy 0.614765 total 916.000000\n",
      "Test auc macro 0.924342 micro 0.948807 \n",
      "Test f1 macro 0.646030 micro 0.670261 \n",
      "Test hamming 0.059990 \n",
      "Test accuracy 0.550336 accuracy total 820.000000 \n",
      "P5 0.260134\n",
      "Iteration 9300: Training loss 0.003304 \n",
      "Train accuracy 0.810000 \n",
      "val auc macro 0.946505 micro 0.970382 \n",
      "Validation accuracy 0.715811 \n",
      "val hamming 0.047858 \n",
      "val accuracy 0.604698 total 901.000000\n",
      "Test auc macro 0.923281 micro 0.946340 \n",
      "Test f1 macro 0.641165 micro 0.667610 \n",
      "Test hamming 0.060609 \n",
      "Test accuracy 0.552349 accuracy total 823.000000 \n",
      "P5 0.258926\n",
      "Iteration 9400: Training loss 0.005816 \n",
      "Train accuracy 0.810000 \n",
      "val auc macro 0.941331 micro 0.967558 \n",
      "Validation accuracy 0.728486 \n",
      "val hamming 0.048529 \n",
      "val accuracy 0.605369 total 902.000000\n",
      "Test auc macro 0.919308 micro 0.946616 \n",
      "Test f1 macro 0.644972 micro 0.669316 \n",
      "Test hamming 0.060145 \n",
      "Test accuracy 0.546980 accuracy total 815.000000 \n",
      "P5 0.259732\n",
      "Iteration 9500: Training loss 0.005578 \n",
      "Train accuracy 0.810000 \n",
      "val auc macro 0.943357 micro 0.967807 \n",
      "Validation accuracy 0.717812 \n",
      "val hamming 0.046515 \n",
      "val accuracy 0.612752 total 913.000000\n",
      "Test auc macro 0.920834 micro 0.943182 \n",
      "Test f1 macro 0.650866 micro 0.673672 \n",
      "Test hamming 0.059319 \n",
      "Test accuracy 0.549664 accuracy total 819.000000 \n",
      "P5 0.256644\n",
      "Starting epoch 8\n",
      "Iteration 9600: Training loss 0.009508 \n",
      "Train accuracy 0.860000 \n",
      "val auc macro 0.940879 micro 0.966498 \n",
      "Validation accuracy 0.735824 \n",
      "val hamming 0.046928 \n",
      "val accuracy 0.606040 total 903.000000\n",
      "Test auc macro 0.919926 micro 0.945160 \n",
      "Test f1 macro 0.648834 micro 0.668179 \n",
      "Test hamming 0.060403 \n",
      "Test accuracy 0.546309 accuracy total 814.000000 \n",
      "P5 0.258926\n",
      "Iteration 9700: Training loss 0.001235 \n",
      "Train accuracy 0.840000 \n",
      "val auc macro 0.943154 micro 0.968758 \n",
      "Validation accuracy 0.730487 \n",
      "val hamming 0.046515 \n",
      "val accuracy 0.610067 total 909.000000\n",
      "Test auc macro 0.922030 micro 0.945346 \n",
      "Test f1 macro 0.645750 micro 0.668367 \n",
      "Test hamming 0.060403 \n",
      "Test accuracy 0.546309 accuracy total 814.000000 \n",
      "P5 0.258523\n",
      "Iteration 9800: Training loss 0.003650 \n",
      "Train accuracy 0.890000 \n",
      "val auc macro 0.945348 micro 0.966394 \n",
      "Validation accuracy 0.735157 \n",
      "val hamming 0.047083 \n",
      "val accuracy 0.612752 total 913.000000\n",
      "Test auc macro 0.920231 micro 0.945961 \n",
      "Test f1 macro 0.628854 micro 0.658185 \n",
      "Test hamming 0.062416 \n",
      "Test accuracy 0.538926 accuracy total 803.000000 \n",
      "P5 0.258792\n",
      "Iteration 9900: Training loss 0.004960 \n",
      "Train accuracy 0.800000 \n",
      "val auc macro 0.936761 micro 0.964260 \n",
      "Validation accuracy 0.726484 \n",
      "val hamming 0.047083 \n",
      "val accuracy 0.608725 total 907.000000\n",
      "Test auc macro 0.918523 micro 0.942466 \n",
      "Test f1 macro 0.627628 micro 0.658737 \n",
      "Test hamming 0.062210 \n",
      "Test accuracy 0.538926 accuracy total 803.000000 \n",
      "P5 0.258255\n",
      "Iteration 10000: Training loss 0.007948 \n",
      "Train accuracy 0.840000 \n",
      "val auc macro 0.943782 micro 0.967226 \n",
      "Validation accuracy 0.731821 \n",
      "val hamming 0.047599 \n",
      "val accuracy 0.603356 total 899.000000\n",
      "Test auc macro 0.922058 micro 0.944622 \n",
      "Test f1 macro 0.630311 micro 0.663629 \n",
      "Test hamming 0.060971 \n",
      "Test accuracy 0.537584 accuracy total 801.000000 \n",
      "P5 0.258389\n",
      "Iteration 10100: Training loss 0.002287 \n",
      "Train accuracy 0.860000 \n",
      "val auc macro 0.939472 micro 0.966537 \n",
      "Validation accuracy 0.740494 \n",
      "val hamming 0.047599 \n",
      "val accuracy 0.605369 total 902.000000\n",
      "Test auc macro 0.919791 micro 0.940766 \n",
      "Test f1 macro 0.632377 micro 0.664965 \n",
      "Test hamming 0.060971 \n",
      "Test accuracy 0.542282 accuracy total 808.000000 \n",
      "P5 0.256376\n",
      "Iteration 10200: Training loss 0.007562 \n",
      "Train accuracy 0.840000 \n",
      "val auc macro 0.944312 micro 0.966903 \n",
      "Validation accuracy 0.727151 \n",
      "val hamming 0.047858 \n",
      "val accuracy 0.606040 total 903.000000\n",
      "Test auc macro 0.923732 micro 0.943582 \n",
      "Test f1 macro 0.640779 micro 0.669504 \n",
      "Test hamming 0.060145 \n",
      "Test accuracy 0.547651 accuracy total 816.000000 \n",
      "P5 0.258121\n",
      "Iteration 10300: Training loss 0.004472 \n",
      "Train accuracy 0.840000 \n",
      "val auc macro 0.948523 micro 0.967617 \n",
      "Validation accuracy 0.731154 \n",
      "val hamming 0.047445 \n",
      "val accuracy 0.609396 total 908.000000\n",
      "Test auc macro 0.921947 micro 0.942838 \n",
      "Test f1 macro 0.638747 micro 0.663080 \n",
      "Test hamming 0.061435 \n",
      "Test accuracy 0.540268 accuracy total 805.000000 \n",
      "P5 0.259463\n",
      "Iteration 10400: Training loss 0.002569 \n",
      "Train accuracy 0.880000 \n",
      "val auc macro 0.933390 micro 0.963375 \n",
      "Validation accuracy 0.727151 \n",
      "val hamming 0.050077 \n",
      "val accuracy 0.599329 total 893.000000\n",
      "Test auc macro 0.911041 micro 0.944198 \n",
      "Test f1 macro 0.630962 micro 0.658910 \n",
      "Test hamming 0.062055 \n",
      "Test accuracy 0.534899 accuracy total 797.000000 \n",
      "P5 0.259597\n",
      "Iteration 10500: Training loss 0.000372 \n",
      "Train accuracy 0.870000 \n",
      "val auc macro 0.948408 micro 0.965647 \n",
      "Validation accuracy 0.720480 \n",
      "val hamming 0.047651 \n",
      "val accuracy 0.606711 total 904.000000\n",
      "Test auc macro 0.921721 micro 0.943795 \n",
      "Test f1 macro 0.637296 micro 0.665528 \n",
      "Test hamming 0.060661 \n",
      "Test accuracy 0.540268 accuracy total 805.000000 \n",
      "P5 0.259866\n",
      "Iteration 10600: Training loss 0.034852 \n",
      "Train accuracy 0.850000 \n",
      "val auc macro 0.950043 micro 0.968166 \n",
      "Validation accuracy 0.721147 \n",
      "val hamming 0.047083 \n",
      "val accuracy 0.609396 total 908.000000\n",
      "Test auc macro 0.918066 micro 0.940586 \n",
      "Test f1 macro 0.626986 micro 0.660426 \n",
      "Test hamming 0.061797 \n",
      "Test accuracy 0.536913 accuracy total 800.000000 \n",
      "P5 0.257852\n",
      "Iteration 10700: Training loss 0.006389 \n",
      "Train accuracy 0.870000 \n",
      "val auc macro 0.949273 micro 0.968636 \n",
      "Validation accuracy 0.729153 \n",
      "val hamming 0.047858 \n",
      "val accuracy 0.604027 total 900.000000\n",
      "Test auc macro 0.917149 micro 0.941153 \n",
      "Test f1 macro 0.633674 micro 0.661556 \n",
      "Test hamming 0.061538 \n",
      "Test accuracy 0.540268 accuracy total 805.000000 \n",
      "P5 0.257450\n",
      "Starting epoch 9\n",
      "Iteration 10800: Training loss 0.002466 \n",
      "Train accuracy 0.870000 \n",
      "val auc macro 0.944971 micro 0.967255 \n",
      "Validation accuracy 0.736491 \n",
      "val hamming 0.046773 \n",
      "val accuracy 0.612081 total 912.000000\n",
      "Test auc macro 0.916110 micro 0.941360 \n",
      "Test f1 macro 0.634063 micro 0.659297 \n",
      "Test hamming 0.062055 \n",
      "Test accuracy 0.538255 accuracy total 802.000000 \n",
      "P5 0.257852\n",
      "Iteration 10900: Training loss 0.000411 \n",
      "Train accuracy 0.850000 \n",
      "val auc macro 0.944766 micro 0.967845 \n",
      "Validation accuracy 0.726484 \n",
      "val hamming 0.046412 \n",
      "val accuracy 0.610738 total 910.000000\n",
      "Test auc macro 0.920606 micro 0.943294 \n",
      "Test f1 macro 0.638162 micro 0.665151 \n",
      "Test hamming 0.060816 \n",
      "Test accuracy 0.542282 accuracy total 808.000000 \n",
      "P5 0.258389\n",
      "Iteration 11000: Training loss 0.001948 \n",
      "Train accuracy 0.870000 \n",
      "val auc macro 0.943964 micro 0.966900 \n",
      "Validation accuracy 0.727151 \n",
      "val hamming 0.046980 \n",
      "val accuracy 0.603356 total 899.000000\n",
      "Test auc macro 0.920527 micro 0.942667 \n",
      "Test f1 macro 0.641479 micro 0.666856 \n",
      "Test hamming 0.060712 \n",
      "Test accuracy 0.544966 accuracy total 812.000000 \n",
      "P5 0.259463\n",
      "Iteration 11100: Training loss 0.038237 \n",
      "Train accuracy 0.870000 \n",
      "val auc macro 0.941785 micro 0.966884 \n",
      "Validation accuracy 0.723149 \n",
      "val hamming 0.047806 \n",
      "val accuracy 0.605369 total 902.000000\n",
      "Test auc macro 0.918256 micro 0.943283 \n",
      "Test f1 macro 0.640206 micro 0.665156 \n",
      "Test hamming 0.061022 \n",
      "Test accuracy 0.544966 accuracy total 812.000000 \n",
      "P5 0.258926\n",
      "Iteration 11200: Training loss 0.009783 \n",
      "Train accuracy 0.860000 \n",
      "val auc macro 0.944130 micro 0.965891 \n",
      "Validation accuracy 0.728486 \n",
      "val hamming 0.048529 \n",
      "val accuracy 0.598658 total 892.000000\n",
      "Test auc macro 0.921736 micro 0.942451 \n",
      "Test f1 macro 0.634320 micro 0.665723 \n",
      "Test hamming 0.060971 \n",
      "Test accuracy 0.544966 accuracy total 812.000000 \n",
      "P5 0.258792\n",
      "Iteration 11300: Training loss 0.000581 \n",
      "Train accuracy 0.860000 \n",
      "val auc macro 0.944337 micro 0.966348 \n",
      "Validation accuracy 0.722482 \n",
      "val hamming 0.048684 \n",
      "val accuracy 0.599329 total 893.000000\n",
      "Test auc macro 0.920068 micro 0.945702 \n",
      "Test f1 macro 0.632930 micro 0.663654 \n",
      "Test hamming 0.061487 \n",
      "Test accuracy 0.547651 accuracy total 816.000000 \n",
      "P5 0.259329\n",
      "Iteration 11400: Training loss 0.014435 \n",
      "Train accuracy 0.870000 \n",
      "val auc macro 0.939468 micro 0.963996 \n",
      "Validation accuracy 0.717812 \n",
      "val hamming 0.049561 \n",
      "val accuracy 0.597315 total 890.000000\n",
      "Test auc macro 0.917794 micro 0.943188 \n",
      "Test f1 macro 0.645078 micro 0.675170 \n",
      "Test hamming 0.059164 \n",
      "Test accuracy 0.553691 accuracy total 825.000000 \n",
      "P5 0.259463\n",
      "Iteration 11500: Training loss 0.002807 \n",
      "Train accuracy 0.880000 \n",
      "val auc macro 0.939670 micro 0.965211 \n",
      "Validation accuracy 0.728486 \n",
      "val hamming 0.048529 \n",
      "val accuracy 0.600000 total 894.000000\n",
      "Test auc macro 0.916545 micro 0.941240 \n",
      "Test f1 macro 0.634523 micro 0.662518 \n",
      "Test hamming 0.061590 \n",
      "Test accuracy 0.544295 accuracy total 811.000000 \n",
      "P5 0.258389\n",
      "Iteration 11600: Training loss 0.003216 \n",
      "Train accuracy 0.860000 \n",
      "val auc macro 0.938639 micro 0.963987 \n",
      "Validation accuracy 0.721815 \n",
      "val hamming 0.049200 \n",
      "val accuracy 0.596644 total 889.000000\n",
      "Test auc macro 0.908063 micro 0.938441 \n",
      "Test f1 macro 0.636109 micro 0.666099 \n",
      "Test hamming 0.060764 \n",
      "Test accuracy 0.540940 accuracy total 806.000000 \n",
      "P5 0.257047\n",
      "Iteration 11700: Training loss 0.006945 \n",
      "Train accuracy 0.850000 \n",
      "val auc macro 0.942012 micro 0.964850 \n",
      "Validation accuracy 0.719146 \n",
      "val hamming 0.048529 \n",
      "val accuracy 0.598658 total 892.000000\n",
      "Test auc macro 0.914513 micro 0.939340 \n",
      "Test f1 macro 0.637677 micro 0.663251 \n",
      "Test hamming 0.061074 \n",
      "Test accuracy 0.540940 accuracy total 806.000000 \n",
      "P5 0.257718\n",
      "Iteration 11800: Training loss 0.002157 \n",
      "Train accuracy 0.830000 \n",
      "val auc macro 0.936148 micro 0.963010 \n",
      "Validation accuracy 0.715811 \n",
      "val hamming 0.049251 \n",
      "val accuracy 0.598658 total 892.000000\n",
      "Test auc macro 0.910162 micro 0.940038 \n",
      "Test f1 macro 0.633938 micro 0.663453 \n",
      "Test hamming 0.061280 \n",
      "Test accuracy 0.539597 accuracy total 804.000000 \n",
      "P5 0.258523\n",
      "Iteration 11900: Training loss 0.046568 \n",
      "Train accuracy 0.820000 \n",
      "val auc macro 0.937731 micro 0.963790 \n",
      "Validation accuracy 0.715143 \n",
      "val hamming 0.046618 \n",
      "val accuracy 0.612081 total 912.000000\n",
      "Test auc macro 0.913912 micro 0.938858 \n",
      "Test f1 macro 0.633607 micro 0.662886 \n",
      "Test hamming 0.061384 \n",
      "Test accuracy 0.544295 accuracy total 811.000000 \n",
      "P5 0.258121\n"
     ]
    }
   ],
   "source": [
    "    with tf.Session(config=config) as sess:\n",
    "        train_writer = tf.summary.FileWriter(opt.log_path + '/train', sess.graph)\n",
    "        test_writer = tf.summary.FileWriter(opt.log_path + '/test', sess.graph)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        if opt.restore:\n",
    "            try:\n",
    "                t_vars = tf.trainable_variables()\n",
    "                save_keys = tensors_key_in_file(opt.save_path)\n",
    "                ss = set([var.name for var in t_vars]) & set([s + \":0\" for s in save_keys.keys()])\n",
    "                cc = {var.name: var for var in t_vars}\n",
    "                # only restore variables with correct shape\n",
    "                ss_right_shape = set([s for s in ss if cc[s].get_shape() == save_keys[s[:-2]]])\n",
    "\n",
    "                loader = tf.train.Saver(var_list=[var for var in t_vars if var.name in ss_right_shape])\n",
    "                loader.restore(sess, opt.save_path)\n",
    "\n",
    "                print(\"Loading variables from '%s'.\" % opt.save_path)\n",
    "                print(\"Loaded variables:\" + str(ss))\n",
    "\n",
    "            except:\n",
    "                print(\"No saving session, using random initialization\")\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        try:\n",
    "            for epoch in range(opt.max_epochs):\n",
    "                print(\"Starting epoch %d\" % epoch)\n",
    "                kf = get_minibatches_idx(len(train), opt.batch_size, shuffle=True)\n",
    "                for _, train_index in kf:\n",
    "                    uidx += 1\n",
    "                    sents = [train[t] for t in train_index]\n",
    "                    x_labels = [train_lab[t] for t in train_index]\n",
    "                    x_labels = np.array(x_labels)\n",
    "                    x_labels = x_labels.reshape((len(x_labels), opt.num_class))\n",
    "\n",
    "                    x_batch, x_batch_mask = prepare_data_for_emb(sents, opt)\n",
    "                    _, loss, step,  = sess.run([train_op, loss_, global_step], feed_dict={x_: x_batch, x_mask_: x_batch_mask, y_: x_labels, keep_prob: opt.dropout, class_penalty_:opt.class_penalty})\n",
    "\n",
    "                    if uidx % opt.valid_freq == 0:\n",
    "                        train_correct = 0.0\n",
    "                        # sample evaluate accuaccy on 100 sample data\n",
    "                        kf_train = get_minibatches_idx(100, opt.batch_size, shuffle=True)\n",
    "                        for _, train_index in kf_train:\n",
    "                            train_sents = [train[t] for t in train_index]\n",
    "                            train_labels = [train_lab[t] for t in train_index]\n",
    "                            train_labels = np.array(train_labels)\n",
    "                            train_labels = train_labels.reshape((len(train_labels), opt.num_class))\n",
    "                            x_train_batch, x_train_batch_mask = prepare_data_for_emb(train_sents, opt)\n",
    "                            train_accuracy = sess.run(accuracy_, feed_dict={x_: x_train_batch, x_mask_: x_train_batch_mask, y_: train_labels, keep_prob: 1.0, class_penalty_:0.0})\n",
    "\n",
    "                            train_correct += train_accuracy * len(train_index)\n",
    "\n",
    "                        train_accuracy = train_correct / 100\n",
    "\n",
    "                        print(\"Iteration %d: Training loss %f \" % (uidx, loss))\n",
    "                        print(\"Train accuracy %f \" % train_accuracy)\n",
    "\n",
    "                        val_correct = 0.0\n",
    "                        val_y = []\n",
    "                        val_logits_list = []\n",
    "                        val_prob_list = []\n",
    "                        val_true_list = []\n",
    "\n",
    "                        kf_val = get_minibatches_idx(len(val), opt.batch_size, shuffle=True)\n",
    "                        for _, val_index in kf_val:\n",
    "                            val_sents = [val[t] for t in val_index]\n",
    "                            val_labels = [val_lab[t] for t in val_index]\n",
    "                            val_labels = np.array(val_labels)\n",
    "                            val_labels = val_labels.reshape((len(val_labels), opt.num_class))\n",
    "                            x_val_batch, x_val_batch_mask = prepare_data_for_emb(val_sents, opt)\n",
    "                            val_accuracy, val_logits, val_probs = sess.run([accuracy_, logits_, prob_], feed_dict={x_: x_val_batch, x_mask_: x_val_batch_mask,\n",
    "                                y_: val_labels, keep_prob: 1.0,\n",
    "                                class_penalty_:0.0                                         })\n",
    "\n",
    "                            val_correct += val_accuracy * len(val_index)\n",
    "                            val_y += np.argmax(val_labels, axis=1).tolist()\n",
    "                            val_logits_list += val_logits.tolist()\n",
    "                            val_prob_list += val_probs.tolist()\n",
    "                            val_true_list += val_labels.tolist()\n",
    "\n",
    "                        val_accuracy = val_correct / len(val)\n",
    "                        val_logits_array = np.asarray(val_logits_list)\n",
    "                        val_prob_array = np.asarray(val_prob_list)\n",
    "                        val_true_array = np.asarray(val_true_list)\n",
    "                        val_auc_list = []\n",
    "\n",
    "                        val_hamming = hamming_loss(y_true=val_true_array, y_pred= np.where(np.array(val_prob_list) > 0.5, 1, 0))\n",
    "                        val_accuracy_fraction = accuracy_score(y_true=val_true_array, y_pred= np.where(np.array(val_prob_list) > 0.5, 1, 0))\n",
    "                        val_accuracy_number = accuracy_score(y_true=val_true_array, y_pred= np.where(np.array(val_prob_list) > 0.5, 1, 0),normalize=False)\n",
    "\n",
    "                        try:\n",
    "                          val_auc_micro = roc_auc_score(y_true=val_true_array, y_score =val_logits_array,average='micro')\n",
    "                          val_auc_macro = roc_auc_score(y_true=val_true_array, y_score =val_logits_array,average='macro')\n",
    "                          for i in range(opt.num_class):\n",
    "                             if np.max(val_true_array[:,i] > 0):\n",
    "                                 val_auc = roc_auc_score(y_true = val_true_array[:,i], y_score= val_logits_array[:,i],)\n",
    "                                 val_auc_list.append(val_auc)\n",
    "                          val_auc_mean = np.mean(val_auc)\n",
    "                          print(\"val auc macro %f micro %f \" % (val_auc_macro, val_auc_micro))\n",
    "                        except:pass\n",
    "\n",
    "                        print(\"Validation accuracy %f \" % val_accuracy)\n",
    "                        print(\"val hamming %f \" % (val_hamming))\n",
    "                        print(\"val accuracy %f total %f\" % (val_accuracy_fraction, val_accuracy_number))\n",
    "\n",
    "                        if True:\n",
    "                            test_correct = 0.0\n",
    "                            test_y = []\n",
    "                            test_logits_list = []\n",
    "                            test_prob_list = []\n",
    "                            test_true_list = []\n",
    "\n",
    "                            kf_test = get_minibatches_idx(len(test), opt.batch_size, shuffle=True)\n",
    "                            for _, test_index in kf_test:\n",
    "                                test_sents = [test[t] for t in test_index]\n",
    "                                test_labels = [test_lab[t] for t in test_index]\n",
    "                                test_labels = np.array(test_labels)\n",
    "                                test_labels = test_labels.reshape((len(test_labels), opt.num_class))\n",
    "                                x_test_batch, x_test_batch_mask = prepare_data_for_emb(test_sents, opt)\n",
    "\n",
    "                                test_accuracy, test_logits, test_probs= sess.run([accuracy_, logits_, prob_],feed_dict={x_: x_test_batch, x_mask_: x_test_batch_mask,y_: test_labels, keep_prob: 1.0, class_penalty_: 0.0})\n",
    "\n",
    "                                test_correct += test_accuracy * len(test_index)\n",
    "\n",
    "                                test_correct += test_accuracy * len(test_index)\n",
    "                                test_y += np.argmax(test_labels, axis=1).tolist()\n",
    "                                test_logits_list += test_logits.tolist()\n",
    "                                test_prob_list += test_probs.tolist()\n",
    "                                test_true_list += test_labels.tolist()\n",
    "                            test_accuracy = test_correct / len(test)\n",
    "                            test_logits_array = np.asarray(test_logits_list)\n",
    "                            test_prob_array = np.asarray(test_prob_list)\n",
    "                            test_true_array = np.asarray(test_true_list)\n",
    "                            test_auc_list = []\n",
    "\n",
    "                            test_hamming = metrics.hamming_loss(y_true=test_true_array, y_pred = np.where(np.array(test_prob_list) > 0.5, 1, 0))\n",
    "\n",
    "                            test_accuracy_fraction = accuracy_score(y_true=test_true_array, y_pred =np.where(np.array(test_prob_list) > 0.5, 1, 0))\n",
    "                            test_accuracy_number = accuracy_score(y_true=test_true_array, y_pred =np.where(np.array(test_prob_list) > 0.5, 1, 0),normalize=False)\n",
    "\n",
    "                            test_f1_micro = micro_f1(test_prob_array.ravel() >0.5, test_true_array.ravel(), )\n",
    "                            test_f1_macro = macro_f1(test_prob_array > 0.5, test_true_array, )\n",
    "                            test_p5 = precision_at_k(test_logits_array, test_true_array , 5)\n",
    "\n",
    "                            try:\n",
    "                              test_auc_micro = roc_auc_score(y_true=test_true_array, y_score =test_logits_array,average='micro')\n",
    "                              test_auc_macro = roc_auc_score(y_true=test_true_array, y_score =test_logits_array,average='macro')\n",
    "\n",
    "                              for i in range(opt.num_class):\n",
    "                                  if np.max(test_true_array[:,i] > 0):\n",
    "                                      test_auc = roc_auc_score(y_true = test_true_array[:,i], y_score= test_logits_array[:,i],)\n",
    "                                      test_auc_list.append(test_auc)\n",
    "\n",
    "                              test_auc_mean = np.mean(test_auc)\n",
    "                              print(\"Test auc macro %f micro %f \" % (test_auc_macro, test_auc_micro))\n",
    "                            except:pass\n",
    "\n",
    "                            print(\"Test f1 macro %f micro %f \" % (test_f1_macro, test_f1_micro))\n",
    "                            print(\"Test hamming %f \" % (test_hamming))\n",
    "                            print(\"Test accuracy %f accuracy total %f \" % (test_accuracy_fraction, test_accuracy_number))\n",
    "                            print(\"P5 %f\" % test_p5)\n",
    "                            # max_test_accuracy = test_accuracy\n",
    "                            #max_test_auc_mean = test_auc_mean\n",
    "                            # print(\"Test accuracy %f \" % test_accuracy)\n",
    "                            #max_test_accuracy = test_accuracy\n",
    "\n",
    "                # print(\"Epoch %d: Max Test accuracy %f\" % (epoch, max_test_accuracy))\n",
    "                #print(\"Epoch %d: Max Test auc %f\" % (epoch, max_test_auc_mean))\n",
    "                saver.save(sess, opt.save_path, global_step=epoch)\n",
    "            #print(\"Max Test accuracy %f \" % max_test_accuracy)\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print('Training interupted')\n",
    "            print(\"Max Test accuracy %f \" % test_accuracy_fraction)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "evalLEAM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "273px",
    "width": "281px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
